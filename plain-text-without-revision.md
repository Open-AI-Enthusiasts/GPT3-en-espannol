
## Resumen

Trabajos recientes han demostrado avances sustanciales en muchas tareas de PNL  y _benchmarks_ mediante el pre-entrernamiento sobre un gran corpus de texto seguido de un _fine-tuning_ en una tarea específica. Si bien la arquitectura es típicamente agnostica frente a la tarea, este método aún requiere un_fine-tuning_ para la trea especifica sobre conjuntos de datos de miles o decenas de miles de ejemplos. En contraste, los humanos generalmente pueden realizar una nueva tarea de lenguaje a partir de solo unos pocos ejemplos o de instrucciones simples, - algo que los sistemas de PNL actuales todavía tienen muchas dificultades para hacer. Aquí mostramos que _escalar _ mejora en gran medida el rendimiento de los modelos de lenguaje agnósticos a las tareas, el perfomance de pocos intentos (_few shots_), y a veces incluso alcanzando niveles de competitividad con enfoques de vanguardia de _fine tuning_ anteriores. Específicamente, entrenamos GPT-3, un modelo de lenguaje autorregresivo con 175 mil millones de parámetros, 10 veces más que cualquier modelo de lenguaje _non-sparse_ anterior, y probamos su rendimiento en la configuración de  pocos intentos. Para todas las tareas, se aplica GPT-3 con tareas y demostraciones de _pocos intentos_ especificadas únicamente a través de la interacción de texto con el modelo, sin actualizaciones de gradiente o _fine tuning_. GPT-3 alcanza un alto rendimiento en diferentes conjuntos de datos de PNL, incluidas las tareas de traducción, responder preguntas, así como varias tareas que requieren razonamiento sobre la marcha o adaptación de dominio, tales como descifrar palabras, usar una palabra nueva en una oración, o realizar aritmética de 3 dígitos. Al mismo tiempo, también identificamos algunos conjuntos de datos donde el aprendizaje de _pocos intentos_ de GPT-3 todavía tiene dificultades, así como algunos conjuntos de datos donde GPT-3 enfrenta problemas metodológicos relacionados con el entrenamiento basado en corpus grandes de la web. Finalmente, encontramos que GPT-3 puede generar muestras de artículos de noticias que los evaluadores humanos tienen dificultades para distinguir de los artículos escritos por humanos. Discutimos los impactos sociales más amplios de este hallazgo y de GPT-3 en general.

## Contenido

1. Introducción
1. Enfoque
	1. Modelo y arquitecturas
	1. Conjunto de datos de entrenamiento
	1. Proceso de formación
	1. Evaluación
1. Resultados
	1. Tareas de modelado de lenguaje, Cloze y finalización
	1. Libro cerrado de preguntas y respuestas
	1. Traducción
	1. Tareas de estilo Winograd
	1. Razonamiento de sentido común
	1. Comprensión lectora
	1. SuperGLUE
	1. NLI
	1. Tareas sintéticas y cualitativas
1. Medición y prevención de la memorización de puntos de referencia
1. Limitaciones
1. Impactos más amplios
	1. Uso indebido de modelos de lenguaje
	2. Justicia, parcialidad y representación
	3. Uso de energía
1. Trabajo relacionado
1. Conclusión


## 1 Introducción 

Los años recientes han presentado una tendencia hacia representaciones lingüísticas pre-entrenadas en los sistemas de PNL, aplicadas en formas cada vez más flexibles y agnósticas para la transferencia aguas abajo. Primero, las representaciones de una sola capa se aprendieron usando wordvectors [MCCD13, PSM14] y se alimentaron a arquitecturas específicas de la tarea, luego se usaron RNN con múltiples capas de representaciones y estado contextual para formar representaciones más fuertes [DL15, MBXS17, PNZtY18] (aunque todavía se aplica totask (arquitecturas específicas) y, más recientemente, los modelos de lenguaje recurrente o transformador preformados [VSP + 17] se han ajustado directamente, eliminando por completo la necesidad de arquitecturas específicas de tareas [RNSS18, DCLT18, HR18]. Este último paradigma ha llevado a progreso sustancial en muchas tareas desafiantes de PNL, como comprensión de lectura, respuesta a preguntas, implicación textual y muchas otras, y ha seguido avanzando en base a nuevas arquitecturas y algoritmos [RSR + 19, LOG + 19, YDY + 19, LCG + 19]. Sin embargo, una limitación importante de este enfoque es que, si bien la arquitectura es independiente de la tarea, todavía existe la necesidad de conjuntos de datos específicos de la tarea y ajustes específicos de la tarea: para lograr un rendimiento sólido en una tarea deseada generalmente se requiere un ajuste fino en un conjunto de datos de miles a cientos de miles de ejemplos específicos de esa tarea. Eliminar esta limitación sería deseable, por varias razones. Primero, desde una perspectiva práctica, la necesidad de un gran conjunto de datos de ejemplos etiquetados para cada nueva tarea limita la aplicabilidad de los modelos de lenguaje. Existe una amplia gama de posibles tareas útiles de lenguaje, que abarcan desde corregir la gramática, generar ejemplos de un concepto abstracto, hasta criticar una historia corta. Para muchas de estas tareas es difícil recopilar un gran conjunto de datos de capacitación supervisada, especialmente cuando el proceso debe repetirse para cada tarea nueva. En segundo lugar, el potencial para explotar correlaciones espurias en los datos de capacitación crece fundamentalmente con la expresividad del modelo y la estrechez de la capacitación. distribución. Esto puede crear problemas para el paradigma de ajuste previo y ajuste fino, donde los modelos están diseñados para ser grandes para absorber información durante el entrenamiento previo, pero luego se ajustan finamente en distribuciones de tareas muy estrechas. Por ejemplo, [HLW + 20] observa que los modelos más grandes no necesariamente generalizan mejor fuera de distribución. Hay evidencia que sugiere que la generalización lograda bajo este paradigma puede ser pobre porque el modelo es demasiado específico para la distribución del entrenamiento y no se generaliza bien fuera de él [YdC + 19, MPL19]. Por lo tanto, el rendimiento de los modelos ajustados en puntos de referencia específicos, incluso cuando es nominalmente a nivel humano, puede exagerar el rendimiento real en la tarea subyacente [GSL + 18, NK19]. En tercer lugar, los humanos no requieren grandes conjuntos de datos supervisados ​​para aprender más tareas de lenguaje: una breve directiva en lenguaje natural (p. ej., "por favor dígame si esta oración describe algo feliz o algo triste") o como máximo un pequeño número de demostraciones (p. ej. "aquí hay dos ejemplos de personas que actúan valientemente; por favor, dé un tercer ejemplo valentía ") es a menudo

figura

Figura 1.1: metaaprendizaje del modelo de lenguaje. Durante el entrenamiento previo sin supervisión, un modelo de lenguaje desarrolla un amplio conjunto de habilidades y habilidades de reconocimiento de patrones. Luego utiliza estas habilidades en el momento de la inferencia para adaptarse rápidamente o reconocer la tarea deseada. Usamos el término "aprendizaje en contexto" para describir el ciclo interno de este proceso, que ocurre dentro del paso directo sobre cada secuencia. Las secuencias en este diagrama no pretenden ser representativas de los datos que vería un modelo durante el pre-entrenamiento, pero tienen la intención de mostrar que a veces hay subtareas repetidas incrustadas en una sola secuencia.

figura

Figura 1.2: Los modelos más grandes hacen un uso cada vez más eficiente de la información en contexto. Mostramos el rendimiento de aprendizaje en contexto en una tarea simple que requiere que el modelo elimine símbolos aleatorios de una palabra, con o sin descripción de la tarea en lenguaje anatómico (ver Sección 3.9. 2) Las "curvas de aprendizaje en contexto" más pronunciadas para modelos grandes demuestran una capacidad mejorada para aprender una tarea a partir de información contextual. Vemos un comportamiento cualitativamente similar en una amplia gama de tareas. Suficiente para permitir que un humano realice una nueva tarea al menos con un grado razonable de competencia. Además de señalar una limitación conceptual en nuestras técnicas actuales de PNL, esta adaptabilidad tiene ventajas prácticas: permite a los humanos mezclar sin problemas o cambiar entre muchas tareas y habilidades, por ejemplo, realizar sumas durante un largo diálogo. Para ser ampliamente útiles, algún día nos gustaría que nuestros sistemas de PNL tengan esta misma fluidez y generalidad. Una ruta potencial para abordar estos problemas es el meta-aprendizaje1, que en el contexto de los modelos de lenguaje significa que el modelo desarrolla un amplio conjunto de habilidades y reconocimiento de patrones. habilidades en el momento del entrenamiento, y luego usa esas habilidades en el tiempo de inferencia para adaptarse rápidamente o reconocer la tarea deseada (ilustrada en la Figura 1.1). El trabajo reciente [RWC + 19] intenta hacer esto a través de lo que llamamos "aprendizaje en contexto", utilizando la entrada de texto de un modelo de lenguaje pre-entrenado como una forma de especificación de tareas: el modelo está condicionado a una instrucción de lenguaje natural y / o un Se espera que pocas demostraciones de la tarea completen más instancias de la tarea simplemente al predecir lo que viene a continuación. Si bien ha mostrado alguna promesa inicial, este enfoque aún logra resultados muy inferiores a la puesta a punto, por ejemplo [RWC + 19] logra solo el 4% en preguntas naturales, e incluso su resultado de 55 CoQa F1 está ahora más de 35 puntos por detrás del estado del arte. El metaaprendizaje claramente requiere una mejora sustancial para ser viable como un método práctico para resolver tareas del lenguaje. Otra tendencia reciente en el modelado del lenguaje puede ofrecer un camino a seguir. En los últimos años, la capacidad de los modelos de lenguaje de transformador ha aumentado sustancialmente, de 100 millones de parámetros [RNSS18], a 300 millones de parámetros [DCLT18], a 1,5 mil millones de parámetros [RWC + 19], a 8 mil millones de parámetros [SPP + 19], 11 mil millones parámetros [RSR + 19], y finalmente 17 mil millones de parámetros [Tur20]. Cada aumento ha traído mejoras en la síntesis de texto y / o en las tareas PNL posteriores, y hay evidencia que sugiere que la pérdida de registros, que se correlaciona bien con muchas tareas posteriores, sigue una tendencia suave de mejora con la escala [KMH + 20]. Dado que el aprendizaje en contexto implica absorber muchas habilidades y tareas dentro de los parámetros del modelo, es plausible que las habilidades de aprendizaje en contexto muestren ganancias similares con escala.1 En el contexto de los modelos de lenguaje, esto a veces se ha denominado "transferencia de tiro cero" , pero este término es potencialmente ambiguo: el método es "cero" en el sentido de que no se realizan actualizaciones de gradiente, pero a menudo implica proporcionar demostraciones de tiempo de inferencia al modelo, por lo que no está realmente aprendiendo de cero ejemplos. Para evitar esta confusión, utilizamos el término "metaaprendizaje" para capturar la estructura del bucle interno / bucle externo del método general, y el término "aprendizaje en contexto" para referirnos al bucle interno del metaaprendizaje. Además, especializamos la descripción a "disparo cero", "disparo único" o "disparo corto", dependiendo de cuántas demostraciones se brinden en el momento de la inferencia. Estos términos tienen la intención de permanecer agnósticos sobre la cuestión de si el modelo aprende nuevas tareas desde cero en el momento de la inferencia o simplemente reconoce los patrones observados durante el entrenamiento; este es un tema importante que discutiremos más adelante en el documento, pero el "meta-aprendizaje" está destinado a abarca ambas posibilidades y simplemente describe la estructura del bucle interno-externo.

FIGURA

Figura 1.3: Rendimiento agregado para los 42 puntos de referencia denominados con precisión Mientras que el rendimiento de tiro cero mejora constantemente con el tamaño del modelo, el rendimiento de tiro corto aumenta más rápidamente, lo que demuestra que los modelos más grandes son más competentes en el aprendizaje en contexto. Consulte la Figura 3.8 para obtener un análisis más detallado de SuperGLUE, una suite estándar NLPbenchmark. En este documento, probamos esta hipótesis al entrenar un modelo de lenguaje autorregresivo de 175 mil millones de parámetros, que llamamos GPT-3, y medir sus habilidades de aprendizaje en contexto. Específicamente, evaluamos GPT-3 en más de dos docenas de conjuntos de datos de PNL, así como varias tareas novedosas diseñadas para probar la adaptación rápida a tareas que probablemente no estén directamente contenidas en el conjunto de entrenamiento. Para cada tarea, evaluamos GPT-3 bajo 3 condiciones: (a) "aprendizaje de pocos disparos", o aprendizaje en contexto donde permitimos tantas demostraciones como quepan en la ventana de contexto del modelo (típicamente 10 a 100), (b ) "Aprendizaje único", donde permitimos solo una demostración, y (c) aprendizaje "cero-disparo", donde no se permiten demostraciones y solo se da una instrucción en lenguaje natural al modelo. GPT-3 también podría evaluarse en principio en la configuración tradicional de ajuste fino, pero lo dejamos para el trabajo futuro. La Figura 1.2 ilustra las condiciones que estudiamos y muestra el aprendizaje de pocos disparos de una tarea simple que requiere que el modelo elimine símbolos extraños de un palabra. El rendimiento del modelo mejora con la adición de una descripción de tareas en lenguaje natural, y con el número de ejemplos en el contexto del modelo, K. El aprendizaje de pocos disparos también mejora dramáticamente con el tamaño del modelo. Aunque los resultados en este caso son particularmente sorprendentes, las tendencias generales con el tamaño del modelo y la cantidad de ejemplos en contexto se mantienen para la mayoría de las tareas que estudiamos. Hacemos hincapié en que estas curvas de "aprendizaje" implican actualizaciones no graduadas o ajuste fino, solo un número creciente de demostraciones dadas como condicionamiento. En general, en las tareas de PNL GPT-3 logra resultados prometedores en la configuración de disparo único y cero, y en el el conjunto de pocos disparos a veces es competitivo o incluso ocasionalmente supera el estado de la técnica (a pesar de que los modelos afinados mantienen el estado de la técnica). Por ejemplo, GPT-3 logra 81.5 F1 en CoQA en la configuración de disparo cero, 84.0 F1 en CoQA en la configuración de un disparo, 85.0 F1 en la configuración de pocos disparos. De manera similar, GPT-3 logra una precisión de 64.3% en TriviaQA en la configuración de disparo cero, 68.0% en la configuración de disparo único y 71.2% en la configuración de disparos pocos, la última de las cuales es de vanguardia en relación con la multa modelos sintonizados que operan en la misma configuración de libro cerrado. GPT-3 también muestra la habilidad de un disparo y pocos disparos en tareas diseñadas para probar la adaptación rápida o el razonamiento sobre la marcha, que incluyen descifrar palabras, realizar operaciones aritméticas y usar palabras nuevas en una oración después de verlas definidas solo una vez. También mostramos que en el entorno de pocos disparos, GPT-3 puede generar artículos de noticias sintéticos que los evaluadores humanos tienen dificultades para distinguir de los artículos generados por humanos. Al mismo tiempo, también encontramos algunas tareas en las que luchas de rendimiento de pocos disparos, incluso en La escala de GPT-3. Esto incluye tareas de inferencia de lenguaje natural como el conjunto de datos ANLI y algunos conjuntos de datos de comprensión de lectura como RACE o QuAC. Al presentar una caracterización amplia de las fortalezas y debilidades de GPT-3, incluidas estas limitaciones, esperamos estimular el estudio del aprendizaje de pocos disparos en modelos de lenguaje y llamar la atención sobre dónde se necesita más progreso. Se puede ver un sentido heurístico de los resultados generales en Figura 1.3, que agrega las diversas tareas (aunque no debe verse como un punto de referencia riguroso o significativo en sí mismo) .5
También llevamos a cabo un estudio sistemático de "contaminación de datos", un problema cada vez mayor al entrenar modelos de alta capacidad en conjuntos de datos como Common Crawl, que potencialmente puede incluir contenido de conjuntos de datos de prueba simplemente porque dicho contenido a menudo existe en la web. En este artículo desarrollamos herramientas sistemáticas para medir la contaminación de datos y cuantificar sus efectos distorsionadores. Aunque descubrimos que la contaminación de datos tiene un efecto mínimo en el rendimiento de GPT-3 en la mayoría de los conjuntos de datos, identificamos algunos conjuntos de datos en los que podría estar inflando resultados, y no informamos los resultados en estos conjuntos de datos o los notamos con un asterisco, dependiendo de La gravedad Además de todo lo anterior, también entrenamos una serie de modelos más pequeños (que van desde 125 millones de parámetros a 13 mil millones de parámetros) para comparar su rendimiento con GPT-3 en la configuración de cero, uno y pocos disparos. En términos generales, para la mayoría de las tareas encontramos una escala relativamente suave con capacidad de modelo en las tres configuraciones; Un patrón notable es que la brecha entre el rendimiento de cero, uno y pocos disparos a menudo aumenta con la capacidad del modelo, lo que quizás sugiere que los modelos más grandes son meta-aprendices más competentes. Finalmente, dado el amplio espectro de capacidades que muestra GPT-3, discuta las preocupaciones sobre el sesgo, la equidad y los impactos sociales más amplios, e intente un análisis preliminar de las características de GPT-3 a este respecto.de este artículo está organizado de la siguiente manera. En la Sección 2, describimos nuestro enfoque y métodos para entrenar GPT-3 y evaluarlo. La Sección 3 presenta los resultados en la gama completa de tareas en los entornos de disparo cero, uno y pocos disparos. La Sección 4 aborda cuestiones de contaminación de datos (superposición de pruebas de tren). La Sección 5 discute las limitaciones de GPT-3. La Sección 6 discute los impactos más amplios. La Sección 7 revisa el trabajo relacionado y la Sección 8 concluye.

2 Enfoque Nuestro enfoque básico previo al entrenamiento, que incluye el modelo, los datos y el entrenamiento, es similar al proceso descrito en [RWC + 19], con una ampliación relativamente sencilla del tamaño del modelo, el tamaño y la diversidad del conjunto de datos y la duración del entrenamiento. Nuestro uso del aprendizaje en contexto también es similar a [RWC + 19], pero en este trabajo exploramos sistemáticamente diferentes entornos para aprender dentro del contexto. Por lo tanto, comenzamos esta sección definiendo y contrastando explícitamente las diferentes configuraciones en las que evaluaremos GPT-3 o en principio podríamos evaluar GPT-3. Esta configuración puede verse como un espectro de la cantidad de datos específicos de la tarea en la que tienden a confiar. Específicamente, podemos identificar al menos cuatro puntos en este espectro (consulte la Figura 2.1 para ver una ilustración): • El ajuste fino (FT) ha sido el enfoque más común en los últimos años e implica la actualización de los pesos de un modelo pre-entrenado mediante la capacitación en Un conjunto de datos supervisado específico para la tarea deseada. Por lo general, se utilizan miles o cientos de miles de ejemplos etiquetados. La principal ventaja del ajuste fino es un fuerte rendimiento en muchos puntos de referencia. Las principales desventajas son la necesidad de un nuevo conjunto de datos grande para cada tarea, la posibilidad de una generalización deficiente fuera de distribución [MPL19], y el potencial de explotar características espurias de los datos de capacitación [GSL + 18, NK19], lo que podría resultar en un Comparación injusta con el desempeño humano. En este trabajo, no ajustamos GPT-3 porque nuestro enfoque se centra en el rendimiento independiente de la tarea, pero GPT-3 puede ajustarse en principio y esta es una dirección prometedora para el trabajo futuro. • Few-Shot (FS) es la término que usaremos en este trabajo para referirnos a la configuración donde el modelo recibe algunas demostraciones de la tarea en el momento de inferencia como condicionamiento [RWC + 19], pero no se permiten actualizaciones de peso. Como se muestra en la Figura 2.1, para un conjunto de datos típico un ejemplo tiene un contexto y una terminación deseada (por ejemplo, una oración en inglés y la traducción al francés), y funciona de manera sencilla al dar K ejemplos de contexto y finalización, y luego un ejemplo final de contexto, con el modelo que se espera que complete. Establezca Wetypically en el rango de 10 a 100, ya que esta es la cantidad de ejemplos que pueden caber en la ventana de contexto del modelo (nctx = 2048). Las principales ventajas de pocos disparos son una reducción importante en la necesidad de datos específicos de la tarea y un potencial reducido para aprender una distribución demasiado estrecha de un conjunto de datos de ajuste fino grande pero estrecho. La principal desventaja es que los resultados de este método hasta ahora han sido mucho peores que los modelos de vanguardia afinados. Además, todavía se requiere una pequeña cantidad de datos específicos de la tarea. Como lo indica el nombre, el aprendizaje de pocos disparos, tal como se describe aquí para los modelos de lenguaje, está relacionado con el aprendizaje de pocos disparos, tal como se usa en otros contextos en ML [HYC01, VBL + 16], ambos implican el aprendizaje basado en una amplia distribución de tareas (en este caso implícito en los datos previos al entrenamiento) y luego adaptarse rápidamente a una nueva tarea. • One-Shot (1S) es lo mismo que pocos disparos, excepto que solo se permite una demostración, además de una descripción del idioma natural de la tarea, como se muestra en la Figura 1. La razón para distinguir un disparo de uno y un disparo a cero (abajo) es que coincide más estrechamente con la forma en que algunas tareas se comunican a los humanos. Por ejemplo, cuando se les pide a los humanos que generen un conjunto de datos en un servicio de trabajador humano (por ejemplo MechanicalTurk), es común dar una demostración de la tarea. Por el contrario, a veces es difícil comunicar el contenido o el formato de una tarea si no se dan ejemplos

FIGURA

Figura 2.1: Disparo cero, un disparo y pocos disparos, en contraste con el ajuste fino tradicional. Los paneles anteriores muestran cuatro métodos para realizar una tarea con un modelo de lenguaje: el ajuste tradicional es el método tradicional, mientras que cero, uno y pocos disparos, que estudiamos en este trabajo, requieren que el modelo realice la tarea solo con Pases hacia adelante en el momento del examen. Por lo general, presentamos el modelo con algunas docenas de ejemplos en la configuración de pocos disparos. Las frases exactas para todas las descripciones de tareas, ejemplos y sugerencias se pueden encontrar en el Apéndice G. • Zero-Shot (0S) es lo mismo que one-shot excepto que no se permiten demostraciones, y el modelo solo proporciona una instrucción en lenguaje natural que describe la tarea. Este método proporciona la máxima comodidad, potencial de robustez y evita las correlaciones espurias (a menos que ocurran de manera muy amplia en el gran corpus de datos previos al entrenamiento), pero también es el entorno más desafiante. En algunos casos, incluso puede ser difícil para los humanos comprender el formato de la tarea sin ejemplos previos, por lo que esta configuración es en algunos casos "injustamente difícil". Por ejemplo, si se le pide a alguien "hacer una tabla de récords mundiales para los 200 m guión ”, esta solicitud puede ser ambigua, ya que puede no estar claro exactamente qué formato debe tener la tabla o qué debe incluirse (e incluso con una aclaración cuidadosa, comprender exactamente lo que se desea puede ser difícil). Sin embargo, para al menos algunos ajustes, el disparo cero es el más cercano a la forma en que los humanos realizan tareas; por ejemplo, en el ejemplo de traducción en la Figura 2.1, un humano probablemente sepa qué hacer solo con la instrucción de texto. La Figura 2.1 muestra los cuatro métodos que utilizan el ejemplo de traducir del inglés al francés. En este documento nos enfocamos en cero disparos, un disparo y pocos disparos, con el objetivo de compararlos no como alternativas competitivas, sino como diferentes configuraciones de problemas que ofrecen una compensación variable entre el rendimiento en puntos de referencia específicos y la eficiencia de la muestra. Resalte los resultados de pocos disparos ya que muchos de ellos solo están ligeramente por detrás de los modelos de vanguardia afinados. Sin embargo, en última instancia, un disparo, o incluso a veces disparo cero, parecen ser las comparaciones más justas con el rendimiento humano, y son objetivos importantes para el trabajo futuro. Las secciones 2.1-2.3 a continuación brindan detalles sobre nuestros modelos, datos de capacitación y proceso de capacitación, respectivamente. La sección 2.4 discute los detalles de cómo hacemos evaluaciones de pocos disparos, un disparo y cero disparos.

TABLA

Model NamenparamsnlayersdmodelnheadsdheadBatch SizeLearning RateGPT-3 Small125M1276812640.5M6.0×10−4GPT-3 Medium350M24102416640.5M3.0×10−4GPT-3 Large760M24153616960.5M2.5×10−4GPT-3 XL1.3B242048241281M2.0×10−4GPT-3 2.7B2.7B32256032801M1.6×10−4GPT-3 6.7B6.7B324096321282M1.2×10−4GPT-3 13B13.0B405140401282M1.0×10−4GPT-3 175B or “GPT-3”175.0B9612288961283.2M0.6×10−4Table 2.1:Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the modelswhich we trained. All models were trained for a total of 300 billion tokens.

Tabla 2.1: Tamaños, arquitecturas e hiperparámetros de aprendizaje (tamaño de lote en tokens y tasa de aprendizaje) de los modelos que hemos entrenado. Todos los modelos fueron entrenados para un total de 300 mil millones de tokens.

2.1 Modelo y arquitecturas Usamos el mismo modelo y arquitectura que GPT-2 [RWC + 19], incluida la inicialización modificada, la prenormalización y la tokenización reversible descritas en este documento, con la excepción de que usamos patrones alternos densos y locales de escasa atención en el capas del transformador, similar al transformador disperso [CGRS19]. Para estudiar la dependencia del rendimiento de ML con el tamaño del modelo, entrenamos 8 tamaños diferentes de modelo, que varían en tres órdenes de magnitud desde 125 millones de parámetros hasta 175 mil millones de parámetros, siendo el último el modelo que llamamos GPT-3. El trabajo previo [KMH + 20] sugiere que con suficientes datos de entrenamiento, la escala de la pérdida de validación debería ser aproximadamente una ley de potencia uniforme como una función del tamaño; Los modelos de entrenamiento de muchos tamaños diferentes nos permiten probar esta hipótesis tanto para la pérdida de validación como para las tareas de lenguaje en sentido descendente. La Tabla 2.1 muestra los tamaños y arquitecturas de nuestros 8 modelos. Herenparams es el número total de parámetros entrenables, nlayers es el número total de capas, dmodelis el número de unidades en cada capa de cuello de botella (siempre tenemos la capa de alimentación directa cuatro veces el tamaño de la capa de cuello de botella, dff = 4 ∗ dmodel), ydhead es la dimensión de Cabeza de atención. Todos los modelos usan una ventana de contexto ofnctx = 2048tokens. Dividimos el modelo en GPU a lo largo de la dimensión de profundidad y anchura para minimizar la transferencia de datos entre nodos. Los parámetros arquitectónicos precisos para cada modelo se eligen en función de la eficiencia computacional y el equilibrio de carga en el diseño de modelos a través de GPU. El trabajo previo [KMH + 20] sugiere que la pérdida de validación no es muy sensible a estos parámetros dentro de un rango razonablemente amplio.2.2 Conjunto de datos de entrenamiento Los conjuntos de datos para modelos de lenguaje se han expandido rápidamente, culminando en el conjunto de datos Common Crawl2 [RSR + 19] que constituyen casi un billón de palabras. Este tamaño de conjunto de datos es suficiente para entrenar a nuestros modelos más grandes sin tener que actualizar la misma secuencia dos veces. Sin embargo, hemos descubierto que las versiones no filtradas o ligeramente filtradas de Common Crawl tienden a tener una calidad más baja que los conjuntos de datos más curados. Por lo tanto, tomamos 3 pasos para mejorar la calidad promedio de nuestros conjuntos de datos: (1) descargamos y filtramos una versión de CommonCrawl basada en la similitud con un rango de corpora de referencia de alta calidad, (2) realizamos una deduplicación difusa a nivel de documento, dentro y entre conjuntos de datos, para evitar la redundancia y preservar la integridad de nuestro conjunto de validación destacado como una medida precisa de sobreajuste, y (3) también agregamos cuerpos de referencia de alta calidad conocidos a la mezcla de capacitación para aumentar CommonCrawl y aumentar su diversidad. de los dos primeros puntos (procesamiento de Common Crawl) se describen en el Apéndice A. Para el tercero, agregamos varios conjuntos de datos de alta calidad seleccionados, incluida una versión ampliada del conjunto de datos de WebText [RWC + 19], recopilados al eliminar enlaces durante un período más largo de tiempo, y descrito por primera vez en [KMH + 20], dos corpus de libros basados ​​en Internet (Books1 y Books2) y Wikipedia en inglés. La Tabla 2.2 muestra la mezcla final de conjuntos de datos que utilizamos en la capacitación. Los datos de CommonCrawl se descargaron de 41 fragmentos de CommonCrawl mensuales que cubren 2016-2019, que constituyen 45 TB de texto sin formato comprimido antes del filtrado y 570 GB después del filtrado, aproximadamente equivalente a 400 mil millones de tokens codificados por pares de bytes. Tenga en cuenta que durante el entrenamiento, los conjuntos de datos no se muestrean en proporción a su tamaño, sino que los conjuntos de datos que vemos como de mayor calidad se muestrean con mayor frecuencia, de modo que los conjuntos de datos de CommonCrawl y Books2 se muestrean menos de una vez durante el entrenamiento, pero los otros conjuntos de datos se muestrearon 2-3 veces. Esto esencialmente acepta una pequeña cantidad de sobreajuste a cambio de datos de capacitación de mayor calidad.

FIGURA

Figura 2.2: Cálculo total utilizado durante el entrenamiento. Según el análisis en Leyes de escala para modelos de lenguaje neural [KMH + 20], entrenamos modelos mucho más grandes en muchos menos tokens de lo que es típico. Como consecuencia, aunque GPT-3 3B es casi 10 veces más grande que RoBERTa-Large (parámetros de 355M), ambos modelos tomaron aproximadamente 50 petaflop / s-días de pre-entrenamiento computarizado. La metodología para estos cálculos se puede encontrar en el Apéndice D

TABLA

Tabla 2.2: Conjuntos de datos utilizados para entrenar GPT-3. "Peso en la mezcla de entrenamiento" se refiere a la fracción de ejemplos durante el entrenamiento que se extraen de un conjunto de datos dado, que intencionalmente no hacemos proporcionales al tamaño del conjunto de datos. Como resultado, cuando entrenamos para 300 mil millones de tokens, algunos conjuntos de datos se ven hasta 3.4 veces durante el entrenamiento, mientras que otros conjuntos de datos se ven menos de una vez.

Una preocupación metodológica importante con los modelos de lenguaje pre-entrenados en una amplia franja de datos de Internet, particularmente modelos grandes con la capacidad de memorizar grandes cantidades de contenido, es la contaminación potencial de las tareas posteriores al tener sus conjuntos de prueba o desarrollo inadvertidamente vistos durante el pre-entrenamiento. Para reducir dicha contaminación, buscamos e intentamos eliminar cualquier superposición con el desarrollo y los conjuntos de pruebas de todos los puntos de referencia estudiados en este documento. Desafortunadamente, un error en el filtrado nos hizo ignorar algunas superposiciones, y debido al costo de la capacitación fue No es factible volver a entrenar el modelo. En la Sección 4 caracterizamos el impacto de las superposiciones restantes, y en el trabajo futuro eliminaremos de forma más agresiva la contaminación de datos. 2.3 Proceso de capacitación Como se encuentra en [KMH + 20, MKAT18], los modelos más grandes pueden usar un tamaño de lote más grande, pero requieren un tamaño más pequeño tasa de aprendizaje. Medimos la escala de ruido de gradiente durante el entrenamiento y la usamos para guiar nuestra elección del tamaño de lote [MKAT18]. La Tabla 2.1 muestra la configuración de parámetros que utilizamos. Para entrenar los modelos más grandes sin quedarse sin memoria, utilizamos una mezcla de paralelismo modelo dentro de cada matriz multiplicada y paralelismo modelo a través de las capas de la red. Todos los modelos fueron capacitados en GPU V100 en parte de un clúster de gran ancho de banda proporcionado por Microsoft. Los detalles del proceso de entrenamiento y la configuración de hiperparámetros se describen en el Apéndice B.9
2.4 Evaluación Para el aprendizaje de pocos disparos, evaluamos cada ejemplo en el conjunto de evaluación extrayendo aleatoriamente K ejemplos del conjunto de entrenamiento de esa tarea como condicionamiento, delimitados por 1 o 2 líneas nuevas dependiendo de la tarea. Para LAMBADA y Storyclozet no hay un conjunto de entrenamiento supervisado disponible, por lo que sacamos ejemplos de acondicionamiento del conjunto de desarrollo y evaluamos el conjunto de prueba. Para Winograd (la versión original, no SuperGLUE) solo hay un conjunto de datos, por lo que extraemos ejemplos de condición directamente de él. Puede ser cualquier valor de 0 a la cantidad máxima permitida por la ventana de contexto del modelo, que es nctx = 2048 para todos los modelos y normalmente se ajusta de 10 a 100 ejemplos . Los valores más grandes de K son usualmente pero no siempre mejores, por lo tanto, cuando hay disponibles un desarrollo separado y un conjunto de prueba, experimentamos con algunos valores de K en el conjunto de desarrollo y luego ejecutamos el mejor valor en el conjunto de prueba. Para algunas tareas (ver Apéndice G) también utilizamos un indicador de lenguaje natural además de (o para K = 0, en lugar de) demostraciones. En las tareas que implican elegir una finalización correcta de varias opciones (opción múltiple), proporcionamos K ejemplos de contexto más correcto finalización, seguido de un ejemplo de contexto solamente, y compare la probabilidad LM de cada finalización. Para la mayoría de las tareas, comparamos la probabilidad por token (para normalizar la longitud), sin embargo, en un pequeño número de conjuntos de datos (ARC, OpenBookQA y RACE) obtenemos un beneficio adicional medido en el conjunto de desarrollo mediante la normalización por la probabilidad incondicional de cada finalización, mediante la computación P (finalización | contexto) P (finalización | contexto de respuesta), cuando el contexto de respuesta es la cadena "Respuesta:" o "A:" y se utiliza para indicar que la finalización debe ser una respuesta, pero de lo contrario es genérica. En tareas que involucran clasificación binaria, dé a las opciones nombres más semánticamente significativos (por ejemplo, "Verdadero" o "Falso" en lugar de 0 o 1) y luego trate la tarea como una opción múltiple; a veces también enmarcamos la tarea de forma similar a lo que hace [RSR + 19] (ver Apéndice G) para más detalles. En tareas con finalización de forma libre, utilizamos la búsqueda de haz con los mismos parámetros que [RSR + 19]: un ancho de haz de 4 y una penalización de longitud de α = 0.6. Calificamos el modelo usando el puntaje de similitud F1, BLEU o coincidencia exacta, dependiendo de lo que sea estándar para el conjunto de datos en cuestión. Los resultados finales se informan en el conjunto de pruebas cuando están disponibles públicamente, para cada tamaño de modelo y configuración de aprendizaje (cero, uno- y pocos disparos). Cuando el conjunto de prueba es privado, nuestro modelo a menudo es demasiado grande para caber en el servidor de prueba, por lo que informamos los resultados en el conjunto de desarrollo. Enviamos al servidor de prueba en un pequeño número de conjuntos de datos (SuperGLUE, TriviaQA, PiQa) donde pudimos hacer que el envío funcione, y enviamos solo los resultados de 200B de pocas tomas e informamos los resultados del conjunto de desarrollo para todo lo demás.



3 Resultados En la Figura 3.1 mostramos curvas de entrenamiento para los 8 modelos descritos en la Sección 2. Para este gráfico también incluimos 6 modelos extrapequeños adicionales con tan solo 100,000 parámetros. Como se observó en [KMH + 20], el rendimiento del modelado del lenguaje sigue una ley de potencia cuando se hace un uso eficiente del cómputo de entrenamiento. Después de extender esta tendencia por dos órdenes de magnitud, observamos solo una ligera (si alguna) desviación de la ley de poder. Uno podría preocuparse de que estas mejoras en la pérdida de entropía cruzada provengan solo de modelar detalles espurios de nuestro corpus de entrenamiento. Sin embargo, veremos en las siguientes secciones que las mejoras en la pérdida de entropía cruzada conducen a ganancias de rendimiento consistentes en todo el espectro de tareas de lenguaje natural en el extranjero. A continuación, evaluamos los 8 modelos descritos en la Sección 2 (los parámetros de 175 mil millones de parámetros GPT-3 y 7 modelos más pequeños) en una amplia gama de conjuntos de datos. Agrupamos los conjuntos de datos en 9 categorías que representan tareas más o menos similares. En la Sección 3.1 evaluamos las tareas de modelado del lenguaje tradicional y las tareas que son similares al modelado del lenguaje, como las tareas de Cloze y las tareas de completar frases / párrafos. En la Sección 3.2 evaluamos las tareas de respuesta a preguntas de "libro cerrado": tareas que requieren el uso de la información almacenada en los parámetros del modelo para responder preguntas de conocimiento general. En la Sección 3.3 evaluamos la capacidad del modelo para traducir entre idiomas (especialmente one-shot y pocos-shot). En la Sección 3.4 evaluamos el rendimiento del modelo en tareas similares al Esquema de Winograd. En la Sección 3.5, evalúe los conjuntos de datos que involucran razonamiento de sentido común o respuestas a preguntas. En la Sección 3.6 evaluamos las tareas de comprensión de lectura, en la Sección 3.7 evaluamos en el conjunto de puntos de referencia SuperGLUE, y en 3.8 exploramos brevemente ILI. Finalmente, en la Sección 3.9, inventamos algunas tareas adicionales diseñadas especialmente para probar las habilidades de aprendizaje en contexto; estas tareas se centran en el razonamiento sobre la marcha, las habilidades de adaptación o la síntesis de texto abierto. Evaluamos todas las tareas en la configuración de pocos disparos, un disparo y cero disparos.

FIGURA

Figura 3.1: Escalado suave del rendimiento con cálculo. El rendimiento (medido en términos de pérdida de validación de entropía cruzada) sigue una tendencia de ley de potencia con la cantidad de cálculo utilizado para el entrenamiento. El comportamiento de la ley de potencia observado en [KMH + 20] continúa durante dos órdenes de magnitud adicionales con solo pequeñas desviaciones de la curva prevista. Para esta figura, excluimos los parámetros de incrustación de cómputo y recuentos de parámetros.

TABLA

Tabla 3.1: Resultados de disparo cero en el conjunto de datos de modelado de lenguaje PTB. Muchos otros conjuntos de datos de modelado de lenguaje común se omiten porque se derivan de Wikipedia u otras fuentes que se incluyen en los datos de entrenamiento de GPT-3. A [RWC + 19] 3.1 Modelado de lenguaje, Cloze y Tareas de finalización En esta sección probamos el rendimiento de GPT-3 en la tarea tradicional de modelado del lenguaje, así como tareas relacionadas que implican predecir una sola palabra de interés, completar una oración o párrafo, o elegir entre posibles compleciones de un texto. .1 Modelado del lenguaje Calculamos la perplejidad de disparo cero en el conjunto de datos Penn Tree Bank (PTB) [MKM + 94] medido en [RWC + 19]. Omitimos las 4 tareas relacionadas con Wikipedia en ese trabajo porque están completamente contenidas en nuestros datos de entrenamiento, y también omitimos el punto de referencia de mil millones de palabras debido a que una gran fracción del conjunto de datos está contenido en nuestro conjunto de entrenamiento. PTB escapa a estos problemas debido a la anterioridad a internet moderno. Nuestro modelo más grande establece un nuevo SOTA en PTB por un margen sustancial de 15 puntos, logrando una perplejidad de 20.50. Tenga en cuenta que, dado que PTB es un conjunto de datos de modelado de lenguaje tradicional, no tiene una separación clara de ejemplos para definir la evaluación de un disparo o de pocos disparos, por lo que medimos solo pruebas de tiro cero. El modelado de dependencias de largo alcance en el texto: se le pide al modelo que repita la última palabra de las oraciones que requieren leer un párrafo de contexto. Recientemente se ha sugerido que la escala continua de los modelos de lenguaje está produciendo rendimientos decrecientes en este punto de referencia difícil. [BHT + 20] reflexiona sobre la pequeña mejora del 1.5% lograda al duplicar el tamaño del modelo entre dos resultados recientes de última generación ([SPP + 19]

tABLA

Tabla 3.2: Rendimiento en las tareas de cierre y finalización. GPT-3 mejora significativamente SOTA en LAMBADA al tiempo que logra un rendimiento respetable en dos conjuntos de datos de predicción de finalización difíciles. A [Tur20] b [RWC + 19] c [LDL19] d [LCH + 20]

Figura 3.2: En LAMBADA, la capacidad de pocos disparos de los modelos de lenguaje da como resultado un fuerte impulso a la precisión. GPT-32.7B supera el parámetro SOTA 17B Turing-NLG [Tur20] en esta configuración, y GPT-3 175B avanza el estado del arte en un 18%. Tenga en cuenta que zero-shot usa un formato diferente de one-shot y pocos-shot como se describe en el texto. Y [Tur20]) y argumentan que "continuar expandiendo los tamaños de hardware y datos por órdenes de magnitud no es el camino a seguir". Encontramos que el camino aún es prometedor y en un entorno de tiro cero, GPT-3 logra un 76% en LAMBADA, una ganancia del 8% sobre el estado del arte anterior. LAMBADA también es una demostración de la flexibilidad del aprendizaje de pocos disparos. Proporciona una forma de abordar un problema que se produce clásicamente con este conjunto de datos. Aunque la terminación en LAMBADA es siempre la última palabra en una oración, un modelo de lenguaje estándar no tiene forma de conocer este detalle. Asigna así la probabilidad no solo al final correcto sino también a otras continuaciones válidas del párrafo. Este problema se ha abordado parcialmente en el pasado con filtros de palabras de detención [RWC + 19] (que prohíben las palabras de "continuación"). La configuración de pocos disparos en su lugar nos permite "enmarcar" la tarea como una prueba de éxito y permite que el modelo de lenguaje infiera a partir de ejemplos que se desea completar exactamente una palabra. Usamos el siguiente formato de relleno en blanco:

Alice was friends with Bob. Alice went to visit her friend.→Bo

George bought some baseball equipment, a ball, a glove, and a.→

Cuando se presentan ejemplos formateados de esta manera, GPT-3 logra una precisión del 86.4% en la configuración de pocos disparos, un aumento de más del 18% con respecto al estado de la técnica anterior. Observamos que el rendimiento de pocos disparos mejora fuertemente con modelize. Si bien esta configuración disminuye el rendimiento del modelo más pequeño en casi un 20%, para GPT-3 mejora la precisión en un 10%. Finalmente, el método de relleno en blanco no es efectivo de un solo disparo, donde siempre funciona peor que el ajuste de cero disparos. Quizás esto se deba a que todos los modelos aún requieren varios ejemplos para reconocer el patrón.

TABLA

Tabla 3.3: Resultados en tres tareas de QA de dominio abierto. GPT-3 se muestra en la configuración de pocos, uno y cero disparos, en comparación con los resultados anteriores de SOTA para la configuración de libro cerrado y dominio abierto. Un resultado de pocos disparos se evalúa en el servidor de prueba dividida de Wiki. Una nota de precaución es que un análisis de la contaminación del conjunto de prueba identificó que una minoría significativa del conjunto de datos LAMBADA parece estar presente en nuestros datos de entrenamiento, sin embargo, el análisis realizado en la Sección 4 sugiere que es insignificante impacton performance.3.1.3 HellaSwag El conjunto de datos HellaSwag [ZHB + 19] implica elegir el mejor final para una historia o conjunto de instrucciones. Los ejemplos fueron extraídos de manera adversa para ser difíciles para los modelos de lenguaje y al mismo tiempo fáciles para los humanos (que logran una precisión del 95.6%) .GPT-3 logra una precisión del 78.1% en la configuración de un solo disparo y una precisión del 79.3% en la configuración de pocos disparos, superando al 75. Precisión del 4% de un modelo de lenguaje de parámetros 1.5B ajustado [ZHR + 19] pero aún bastante más bajo que el SOTA general del 85.6% logrado por el modelo de tareas múltiples ajustado ALUM.3.1.4 StoryCloze A continuación evaluaremos GPT- 3 en el conjunto de datos StoryCloze 2016 [MCH + 16], que implica seleccionar la frase final correcta para historias largas de cinco oraciones. Aquí GPT-3 logra 83.2% en la configuración de disparo cero y 87.7% en la configuración de pocos disparos (con K = 70). Esto sigue siendo un 4.1% más bajo que el SOTA ajustado usando un modelo basado en BERT [LDL19], pero mejora los resultados anteriores de disparo cero en aproximadamente un 10% .3.2 Respuesta a preguntas de libro cerrado En esta sección medimos la capacidad de GPT-3 para responder preguntas sobre amplio conocimiento factual. Debido a la inmensa cantidad de posibles consultas, esta tarea normalmente se ha abordado mediante el uso de un sistema de recuperación de información para encontrar texto relevante en combinación con un modelo que aprende a generar una respuesta dada la pregunta y el texto recuperado. Dado que esta configuración permite que un sistema busque y condicione el texto que potencialmente contiene la respuesta, se denomina "libro abierto". [RRS20] demostró recientemente que un modelo de lenguaje grande puede funcionar sorprendentemente bien respondiendo las preguntas sin condicionar la información auxiliar. Denotan esta configuración de evaluación más restrictiva como "libro cerrado". Su trabajo sugiere que incluso los modelos de mayor capacidad podrían funcionar incluso mejor y probamos esta hipótesis con GPT-3. Evaluamos GPT-3 en los 3 conjuntos de datos en [RRS20]: Preguntas naturales [KPR + 19], WebQuestions [BCFL13] y TriviaQA [JCWZ17], utilizando las mismas divisiones. Tenga en cuenta que, además de que todos los resultados se encuentran en la configuración de libro cerrado, nuestro uso de evaluaciones de pocos disparos, de una sola vez y de cero disparos representa una configuración aún más estricta que el trabajo previo de control de calidad de libro cerrado: además del contenido externo que no está permitido , el ajuste fino en el conjunto de datos de preguntas y respuestas en sí tampoco está permitido.

FIGURA

Figura 3.3: En TriviaQA, el rendimiento de GPT3 crece sin problemas con el tamaño del modelo, lo que sugiere que los modelos de lenguaje continúan absorbiendo conocimiento a medida que aumenta su capacidad. El rendimiento de un disparo y pocos disparos genera una ganancia significativa sobre el comportamiento de disparo cero, igualando y superando el rendimiento del modelo de dominio abierto ajustado SOTA, RAG [LPP + 20

Los resultados para GPT-3 se muestran en la Tabla 3.3. En TriviaQA, logramos un 64,3% en la configuración de disparo cero, un 68,0% en la configuración de un solo disparo y un 71,2% en la configuración de pocos disparos. El resultado de disparo cero ya supera al T5-11B afinado en un 14.2%, y también supera a una versión con predicción de intervalo de preguntas y respuestas durante el pre-entrenamiento en un 3.8%. El resultado de una sola vez mejora en un 3,7% y coincide con el SOTA para un sistema de control de calidad de dominio abierto que no solo afina sino que también utiliza un mecanismo de recuperación aprendido sobre un índice de vector denso de parámetro de 15.3B de 21M documentos [LPP + 20]. El resultado de pocos disparos de GPT-3 mejora aún más el rendimiento otro 3.2% más allá de esto. En WebQuestions (WebQs), GPT-3 alcanza el 14.4% en la configuración de disparo cero, el 25.3% en la configuración de un disparo y el 41.5% en los pocos ajuste de disparo. Esto se compara con el 37.4% para T5-11B ajustado y el 44.7% para T5-11B + SSM ajustado, que utiliza un procedimiento de pre-entrenamiento específico de preguntas y respuestas. GPT-3 en la configuración de pocos disparos se acerca al rendimiento de los modelos ajustados de última generación. Cabe destacar que, en comparación con TriviaQA, WebQS muestra una ganancia mucho mayor con el tiro cero tofew-shot (y, de hecho, su rendimiento de tiro cero y one-shot son pobres), lo que sugiere que las preguntas de WebQs y / o el estilo de sus respuestas son fuera de distribución para GPT-3. Sin embargo, GPT-3 parece capaz de adaptarse a esta distribución, recuperando un rendimiento sólido en la configuración de pocos disparos. En Preguntas Naturales (NQ) GPT-3 alcanza el 14.6% en la configuración de disparo cero, el 23.0% en la configuración de disparo único, y 29.9% en la configuración de pocos disparos, en comparación con 36.6% para T5 11B + SSM afinado. Similar a WebQS, la gran ganancia de cero disparos a pocos disparos puede sugerir un cambio de distribución, y también puede explicar el rendimiento menos competitivo en comparación con TriviaQA y WebQS. En particular, las preguntas en NQ tienden hacia un conocimiento muy fino en Wikipedias específicamente que podría estar probando los límites de la capacidad de GPT-3 y la amplia distribución de preentrenamiento. En general, en uno de los tres conjuntos de datos, el one-shot de GPT-3 coincide con el open- ajuste de dominio SOTA. En los otros dos conjuntos de datos, se aproxima al rendimiento del SOTA de libro cerrado a pesar de no utilizar el ajuste fino. En los 3 conjuntos de datos, encontramos que el rendimiento escala muy suavemente con el tamaño del modelo (Figura 3.3 y Apéndice H, Figura H.7), posiblemente reflejando la idea de que la capacidad del modelo se traduce directamente en más "conocimiento" absorbido en los parámetros del modelo.

3.3 Traducción Para GPT-2 se utilizó un filtro en una colección de documentos multilingües para producir un conjunto de datos solo en inglés debido a problemas de capacidad. Incluso con este filtrado, GPT-2 mostró cierta evidencia de capacidad multilingüe y se realizó de manera no trivial al traducir entre francés e inglés a pesar de solo entrenar en 10 megabytes del texto francés restante. Dado que aumentamos la capacidad en más de dos órdenes de magnitud de GPT-2 a GPT-3, también ampliamos el alcance del conjunto de datos de capacitación para incluir una mayor representación de otros idiomas, aunque esto sigue siendo un área para una mejora adicional. Como se discutió en 2.2, la mayoría de nuestros datos se derivan de Common Crawl sin procesar con solo un filtrado basado en la calidad. Aunque los datos de capacitación de GPT-3 siguen siendo principalmente en inglés (93% por conteo de palabras), también incluyen el 7% del texto en otros idiomas. Estos idiomas están documentados en el material complementario. Para comprender mejor la capacidad de traducción, también ampliamos nuestro análisis para incluir dos idiomas adicionales comúnmente estudiados, alemán y rumano. Los enfoques de traducción automática no supervisados ​​existentes a menudo combinan el entrenamiento previo en un par de conjuntos de datos monolingües con traducción inversa [SHB15] para unir los dos idiomas en de manera controlada Por el contrario, GPT-3 aprende de una gran cantidad de datos de entrenamiento que mezclan muchos idiomas de manera natural, combinándolos a nivel de palabra, oración y documento. GPT-3 también utiliza un único objetivo de entrenamiento que no está personalizado ni diseñado para ninguna tarea en particular. Sin embargo, nuestra configuración de uno / pocos disparos no es estrictamente comparable al trabajo previo sin supervisión, ya que hacen uso de una pequeña cantidad de ejemplos emparejados (1 o 64). Esto corresponde a hasta una o dos páginas de datos de capacitación en contexto. Los resultados se muestran en la Tabla 3.4. Zero-shot GPT-3, que solo recibe una descripción de la tarea en lenguaje natural, todavía tiene un rendimiento inferior a los resultados recientes de NMT sin supervisión. Sin embargo, proporcionar solo una demostración de ejemplo para cada tarea de traducción mejora el rendimiento en más de 7 BLEU y se acerca al rendimiento competitivo con trabajos previos. . GPT-3 tiene un sesgo notable en su rendimiento dependiendo de la dirección del idioma. Para los tres idiomas de entrada estudiados, GPT-3 supera significativamente el trabajo de NMT no supervisado previo al traducir al inglés, pero tiene un rendimiento inferior al traducir en la otra dirección. El rendimiento en En-Ro es notablemente superior a 10 BLEU peor que el trabajo de NMT sin supervisión anterior. Esto podría ser una debilidad debido a la reutilización del BPEtokenizer de nivel de byte de GPT-2 que fue desarrollado para un conjunto de datos de entrenamiento casi completamente en inglés. Tanto para Fr-En como para De-En, pocos disparos GPT-3 superan el mejor resultado supervisado que pudimos encontrar, pero debido a nuestra falta de familiaridad con la literatura y la apariencia de que estos son puntos de referencia no competitivos, no sospechamos que esos resultados representen el verdadero estado de Para Ro-En, pocos disparos GPT-3 se realizan dentro de 0.5 BLEU del SOTA general que se logra mediante una combinación de pre-entrenamiento no supervisado, ajuste fino supervisado en ejemplos etiquetados 608K y traducción inversa [LHCG19b]. Finalmente, en todos los pares de idiomas y En las tres configuraciones (cero, uno y pocos disparos), existe una tendencia suave de mejora con la capacidad del modelo. Esto se muestra en la Figura 3.4 en el caso de resultados de pocos disparos, y la escala para todos los ajustes de tres se muestra en el Apéndice H.

TABLA

Tabla 3.4: Pocos disparos GPT-3 supera el trabajo de NMT no supervisado anterior por 5 BLEU al traducir al inglés, lo que refleja su fuerza como LM en inglés. Reportamos puntajes de BLEU en WMT'14 Fr↔En, WMT'16 De↔En y WMT '16 conjuntos de datos Ro↔En medidos por multi-bleu.perl con la tokenización de XLM para comparar más estrechamente con el trabajo NMT no supervisado previo. SacreBLEUf [Pos18] resultados reportados en el Apéndice H. El subrayado indica un SOTA no supervisado o de pocos disparos, en negrita indica un SOTA supervisado con relativa confianza. A [EOAG18] b [DHKH14] c [WXH + 18] d [oR16] e [LGG +20] f [Firma SacreBLEU: BLEU + case.mixed + numrefs.1 + smooth.exp + tok.intl + versión.1.2 .20]

FIGURA

Figura 3.4: Rendimiento de traducción de pocos disparos en 6 pares de idiomas a medida que aumenta la capacidad del modelo. Hay una tendencia constante de mejora en todos los conjuntos de datos a medida que el modelo escala, y también hay una tendencia a que la traducción al inglés sea más sólida que la traducción del inglés.

TABLA

Tabla 3.5: Resultados sobre la versión WSC273 de los esquemas de Winograd y el conjunto de datos adversario de Winogrande. Consulte la Sección 4 para obtener detalles sobre la posible contaminación del conjunto de pruebas de Winograd. A [SBBC19] b [LYN + 20]

FIGURA

Figura 3.5: Rendimiento de cero, uno y pocos disparos en el conjunto de datos adversario de Winogrande a medida que escala la capacidad del modelo. El escalado es relativamente suave con las ganancias del aprendizaje de pocos disparos aumentando con el tamaño del modelo, y GPT-3 175 de pocos disparos. con un RoBERTA-grande afinado.

POSIBLE REPETIDO

cada tarea de traducción mejora el rendimiento en más de 7 BLEU y se acerca al rendimiento competitivo con el trabajo anterior. GPT-3 en la configuración completa de pocos disparos mejora aún más otros 4 BLEU, lo que resulta en un rendimiento promedio similar al trabajo NMT sin supervisión previa. GPT-3 tiene un sesgo notable en su rendimiento dependiendo de la dirección del idioma. Para los tres idiomas de entrada estudiados, GPT-3 supera significativamente el trabajo de NMT no supervisado previo al traducir al inglés, pero tiene un rendimiento inferior al traducir en la otra dirección. El rendimiento en En-Ro es notablemente superior a 10 BLEU peor que el trabajo de NMT sin supervisión anterior. Esto podría ser una debilidad debido a la reutilización del BPEtokenizer de nivel de byte de GPT-2 que fue desarrollado para un conjunto de datos de entrenamiento casi completamente en inglés. Tanto para Fr-En como para De-En, pocos disparos GPT-3 superan el mejor resultado supervisado que pudimos encontrar, pero debido a nuestra falta de familiaridad con la literatura y la apariencia de que estos son puntos de referencia no competitivos, no sospechamos que esos resultados representen el verdadero estado de Para Ro-En, pocos disparos GPT-3 se realizan dentro de 0.5 BLEU del SOTA general que se logra mediante una combinación de pre-entrenamiento no supervisado, ajuste fino supervisado en ejemplos etiquetados 608K y traducción inversa [LHCG19b]. Finalmente, en todos los pares de idiomas y En las tres configuraciones (cero, uno y pocos disparos), existe una tendencia suave de mejora con la capacidad del modelo. Esto se muestra en la Figura 3.4 en el caso de resultados de pocos disparos, y la escala de todos los tres ajustes se muestra en el Apéndice H.3.4 Tareas al estilo de Winograd El Desafío de esquemas de Winograd [LDM12] es una tarea clásica en PNL que implica determinar qué palabra se pronuncia a, cuando el pronombre es gramaticalmente ambiguo pero semánticamente inequívoco para un humano. Recientemente, los modelos de idioma ajustados han logrado un rendimiento casi humano en el conjunto de datos original de Winograd, pero versiones más difíciles

TABLA

Tabla 3.6: Resultados de GPT-3 en tres tareas de razonamiento de sentido común, PIQA, ARC y OpenBookQA. GPT-3 Pocos resultados ShotPIQA se evalúa en el servidor de prueba. Consulte la Sección 4 para obtener detalles sobre posibles problemas de contaminación en el conjunto de pruebas PIQA.

FIGURA

Figura 3.6: Resultados de GPT-3 en PIQA en la configuración de disparo cero, un disparo y pocos disparos. El modelo más grande logra un puntaje en el conjunto de desarrollo en las tres condiciones que excede el mejor puntaje registrado en la tarea

Figura 3.6: Resultados de GPT-3 en PIQA en la configuración de disparo cero, un disparo y pocos disparos. El modelo más grande logra un puntaje en el conjunto de desarrollo en las tres condiciones que excede el mejor puntaje registrado en la tarea.

tales como el conjunto de datos de Winogrande extraído de manera conflictiva [SBBC19] todavía tienen un retraso significativo en el rendimiento humano. Probamos el rendimiento de GPT-3 tanto en Winograd como en Winogrande, como es habitual en la configuración de cero, uno y pocos disparos. En Winograd probamos GPT-3 en el conjunto original de 273 esquemas de Winograd, usando la misma "evaluación parcial" método descrito en [RWC + 19]. Tenga en cuenta que esta configuración difiere ligeramente de la tarea de WSC en el punto de referencia SuperGLUE, que se presenta como clasificación binaria y requiere la extracción de la entidad para convertirla al formulario descrito en esta sección. OnWinograd GPT-3 alcanza el 88.3%, 89.7% y 88.6% en las configuraciones de tiro cero, uno y pocos disparos, sin mostrar un aprendizaje claro en el contexto, pero en todos los casos logrando resultados sólidos solo unos pocos puntos por debajo del estado Rendimiento humano estimado y de última generación. Observamos que el análisis de contaminación encontró algunos esquemas de Winograd en los datos de entrenamiento, pero esto parece tener solo un pequeño efecto en los resultados (ver Sección 4). En el conjunto de datos de Winogrande más difícil, encontramos ganancias en el aprendizaje en contexto: GPT-3 logra 70.2% en la configuración de disparo cero, 73.2% en la configuración de disparo único y 77.7% en la configuración de disparo único. A modo de comparación, un modelo ROBERTA ajustado logra el 79%, el 84,6% de vanguardia se logra con un modelo de alta capacidad (T5) ajustado y el rendimiento humano en la tarea según lo informado por [SBBC19] es del 94,0%.

3.5 Razonamiento de sentido común A continuación, consideramos tres conjuntos de datos que intentan capturar el razonamiento físico o científico, a diferencia de la finalización de oraciones, la comprensión de lectura o la respuesta a preguntas de conocimiento amplio. El primero, PhysicalQA (PIQA) [BZB + 19], hace preguntas de sentido común sobre cómo funciona el mundo físico y está pensado como una sonda de comprensión del mundo. GPT-3 logra 81.0% de precisión de tiro cero, 80.5% de precisión de un disparo y 82.8% de precisión de tiro corto (la última medida en el servidor de prueba de PIQA). Esto se compara favorablemente con el 79,4% de precisión del estado del arte de un

TABLA

Tabla 3.7: Resultados en tareas de comprensión lectora. Todos los puntajes son F1, excepto los resultados de RACE que informan la precisión. A [JZC + 19] b [JN20] c [AI19] d [QIA20] e [SPP + 19]

RoBERTa afinado. PIQA muestra una escala relativamente baja con el tamaño del modelo y aún es un 10% peor que el rendimiento humano, pero el resultado de pocos disparos e incluso cero disparos de GPT-3 supera el estado del arte actual. Nuestro análisis marcó PIQA para un posible problema de contaminación de datos (a pesar de las etiquetas de prueba ocultas) y, por lo tanto, marcamos el resultado de forma conservadora con un asterisco. Consulte la Sección 4 para obtener más detalles. ARC [CCE + 18] es un conjunto de datos de preguntas de opción múltiple recopiladas de los exámenes de ciencias de 3 ° a 9 ° grado. En la versión "Desafío" del conjunto de datos que se ha filtrado a preguntas que los métodos estadísticos o de recuperación de información simples no pueden responder correctamente, GPT-3 logra una precisión del 51.4% en el ajuste de disparo cero, 53.2% en el ajuste de un disparo, y 51.5% en la configuración de pocos disparos. Esto se acerca al rendimiento de una línea de base RoBERTa afinada (55.9%) de UnifiedQA [KKS + 20]. En la versión "fácil" del conjunto de datos (preguntas que cualquiera de los enfoques de línea de base mencionados respondieron correctamente), GPT-3 logra 68.8%, 71.2% y 70.1% que excede ligeramente una línea de base RoBERTa ajustada de [KKS + 20]. Sin embargo, estos dos resultados siguen siendo mucho peores que el SOTA general alcanzado por el UnifiedQA que supera los resultados de pocos disparos de GPT-3 en un 27% en el conjunto de desafío y 22% en el conjunto fácil. En OpenBookQA [MCKS18], GPT-3 mejora significativamente de cero a pocos ajustes de disparo, pero todavía es más de 20 puntos por debajo del SOTA general. El rendimiento de pocos disparos de GPT-3 es similar a una línea de base BERT Large afinada en la tabla de clasificación. En general, el aprendizaje en contexto con GPT-3 muestra resultados mixtos en tareas de razonamiento de sentido común, con solo ganancias pequeñas e inconsistentes observadas en uno y pocos. ajustes de aprendizaje de disparo para PIQA y ARC, pero se observa una mejora significativa en OpenBookQA. GPT-3 establece SOTA en el nuevo conjunto de datos PIQA en todas las configuraciones de evaluación. 3.6 Comprensión de lectura A continuación, evaluamos GPT-3 en la tarea de comprensión de lectura. Utilizamos un conjunto de 5 conjuntos de datos que incluyen formatos de respuesta abstractos, de opción múltiple y basados ​​en el alcance, tanto en el diálogo como en la configuración de preguntas individuales. Observamos una amplia difusión en el rendimiento de GPT-3 en estos conjuntos de datos que sugiere una capacidad variable con diferentes formatos de respuesta. En general, observamos que GPT-3 está a la par con las líneas de base iniciales y los primeros resultados entrenados utilizando representaciones contextuales en cada conjunto de datos respectivo. GPT-3 se desempeña mejor (dentro de los 3 puntos de la línea de base humana) en CoQA [RCM19] un archivo de datos de conversación de forma libre y peor desempeño (13 F1 debajo de una línea de base ELMo) en QuAC [CHI + 18] un conjunto de datos que requiere modelar actos de diálogo estructurados y responder a selecciones de intervalo de interacciones profesor-alumno. En DROP [DWD + 19], un conjunto de datos que prueba el razonamiento discreto y la aritmética en el contexto de la comprensión de lectura, GPT-3 en un entorno de pocos disparos supera la línea de base de TRB ajustada del documento original, pero todavía está muy por debajo del rendimiento y el estado humano. enfoques de vanguardia que aumentan las redes neuronales con sistemas simbólicos [RLL + 19]. En SQuAD 2.0 [RJL18], GPT-3 demuestra sus capacidades de aprendizaje de pocos disparos, mejorando en casi 10 F1 (a 69.8) en comparación con un ajuste de disparo cero. Esto le permite superar ligeramente el mejor resultado ajustado en el papel original. En RACE [LXL + 17], un conjunto de datos de opción múltiple de exámenes de inglés de secundaria y preparatoria, GPT-3 se desempeña relativamente débilmente y solo es competitivo con el primer trabajo utilizando representaciones contextuales y todavía está 45% por detrás de SOTA.

FIGRA

Figura 3.7: Resultados de GPT-3 en la tarea de comprensión de lectura CoQA. GPT-3 175B logra 85 F1 en la configuración de pocos disparos, solo unos pocos puntos por detrás del rendimiento humano medido y los modelos de vanguardia afinados. El rendimiento de disparo cero y un disparo está a unos pocos puntos de distancia, y las ganancias de pocos disparos son mayores para los modelos más grandes.

TABLA

Tabla 3.8: Rendimiento de GPT-3 en SuperGLUE en comparación con líneas de base ajustadas y SOTA. Todos los resultados se informan en el conjunto de prueba. GPT-3 pocos disparos recibe un total de 32 ejemplos dentro del contexto de cada tarea y no realiza actualizaciones de gradiente.

FIGURA

Figura 3.8: El rendimiento en SuperGLUE aumenta con el tamaño del modelo y la cantidad de ejemplos en contexto. Un valor de K = 32 significa que se mostró a nuestro modelo 32 ejemplos por tarea, para un total de 256 ejemplos divididos entre las 8 tareas en SuperGLUE. Informamos los valores de GPT-3 en el conjunto de desarrollo, por lo que nuestros números no son directamente comparables con las líneas de referencia punteadas (los resultados de nuestro conjunto de pruebas se encuentran en la Tabla 3.8). El modelo de referencia BERT-Large se ajustó en el conjunto de entrenamiento SuperGLUE (ejemplos de 125K), mientras que BERT ++ se ajustó primero en MultiNLI (ejemplos de 392K) y SWAG (ejemplos de 113K) antes de afinar aún más el conjunto de entrenamiento SuperGLUE (para un total de 630K ejemplos de ajuste). Encontramos que la diferencia en el rendimiento entre BERT-Large y BERT ++ es aproximadamente equivalente a la diferencia entre GPT-3 con un ejemplo por contexto versus ocho ejemplos por contexto.

MultiRC, probamos un nuevo conjunto de ejemplos para usar en el contexto de cada problema. Para WSC y MultiRC, utilizamos el mismo conjunto de ejemplos extraídos al azar del conjunto de capacitación como contexto para todos los problemas que evaluamos. Observamos una amplia gama en el rendimiento de GPT-3 en todas las tareas. En COPA y ReCoRD GPT-3 logra un rendimiento cercano a SOTA en la configuración de un disparo y pocos disparos, con COPA cayendo solo un par de puntos y logrando el segundo lugar en la tabla de clasificación, donde el primer lugar lo ocupa un parámetro ajustado de 11 mil millones modelo (T5). En WSC, el rendimiento sigue siendo relativamente fuerte, alcanzando el 80.1% en el entorno de pocos disparos (tenga en cuenta que GPT-3 logra el 88.6% en el conjunto de datos original de Winograd como se describe en la Sección 3.4). En BoolQ, MultiRC y RTE, el rendimiento es razonable y coincide aproximadamente con el de un BERT-Large afinado. En CB, vemos signos de vida al 75.6% en el entorno de pocos disparos. El WiC es un punto débil notable con un rendimiento de pocos disparos al 49.4% (al azar). Probamos varias frases y formulaciones diferentes para WiC (que implica determinar si una palabra se usa con el mismo significado en dos oraciones), ninguna de las cuales fue capaz de lograr un rendimiento sólido. Esto sugiere un fenómeno que se volverá más claro en la siguiente sección (que discute el punto de referencia ANLI): GPT-3 parece ser débil en la configuración de pocos disparos o un disparo en algunas tareas que implican comparar dos oraciones o fragmentos, por ejemplo si una palabra se usa de la misma manera en dos oraciones (WiC), ya sea que una oración sea una paráfrasis de otra, o si una oración implica otra. Esto también podría explicar los puntajes relativamente bajos para RTE y CB, que también siguen este formato. A pesar de estas debilidades, GPT-3 todavía supera a un BERT-sintonizado fino en cuatro de las ocho tareas y en dos tareas GPT-3 está cerca del estado de la técnica de un modelo de parámetro de 11 mil millones afinado. observamos que la puntuación SuperGLUE de pocos disparos mejora constantemente con el tamaño del modelo y con el número de ejemplos en el contexto que muestran beneficios crecientes del aprendizaje en contexto (Figura 3.8). Escalamos hasta 32 ejemplos por tarea, después de lo cual, ejemplos adicionales no encajarán de manera confiable en nuestro contexto. Al barrer los sobrevalores de K, encontramos que GPT-3 requiere menos de ocho ejemplos totales por tarea para superar un puntaje general SuperGLUE ajustado BERT-Largeon.

3.8 NLIN Inferencia del lenguaje natural (NLI) [Fyo00] se refiere a la capacidad de comprender la relación entre dos oraciones. En la práctica, esta tarea generalmente se estructura como un problema de clasificación de dos o tres clases donde el modelo clasifica

FIGURA

Figura 3.9: Rendimiento de GPT-3 en ANLI Ronda 3. Los resultados están en el conjunto de desarrollo, que tiene solo 1500 ejemplos y, por lo tanto, tiene una alta varianza (estimamos una desviación estándar de 1.2%). Encontramos que los modelos más pequeños rondan la posibilidad aleatoria, mientras que el GPT-3 175B de pocos disparos cierra casi la mitad de la brecha de la probabilidad aleatoria a SOTA. Los resultados de las rondas 1 y 2 de ANLI se muestran en el apéndice.

si la segunda oración se deduce lógicamente de la primera, contradice la primera oración o es posiblemente verdadera (neutral). SuperGLUE incluye un conjunto de datos NLI, RTE, que evalúa la versión binaria de la tarea. En RTE, solo la versión más grande de GPT-3 funciona de manera convincente mejor que aleatoria (56%) en cualquier configuración de evaluación, pero en una configuración de pocos disparos GPT-3 funciona de manera similar a un BERT Grande ajustado de una sola tarea. También evaluamos el conjunto de datos de Inferencia Adversarial del Lenguaje Natural (ANLI) recientemente introducido [NWD + 19]. ANLI es un conjunto de datos difícil que emplea una serie de preguntas de inferencia de lenguaje natural extraídas de manera desmesurada en tres rondas (R1, R2 y R3). Similar a RTE, todos nuestros modelos más pequeños que GPT-3 tienen un rendimiento casi exactamente aleatorio en ANLI, incluso en la configuración de pocos disparos (∼33%), mientras que GPT-3 en sí muestra signos de vida en la Ronda 3. Resultados para ANLI R3 se resaltan en la Figura 3.9 y los resultados completos de todas las rondas se pueden encontrar en el Apéndice H. Estos resultados tanto en RTE como en ANLI sugieren que NLI sigue siendo una tarea muy difícil para los modelos de lenguaje y solo están comenzando a mostrar signos de progreso. y Tareas Cualitativas Una forma de probar el rango de habilidades de GPT-3 en la configuración de pocos disparos (o cero y un disparo) es darle tareas que requieran que realice un razonamiento computacional simple sobre la marcha, reconocer un patrón novedoso que es poco probable que haya ocurrido en el entrenamiento o que se adapte rápidamente a una tarea inusual. Diseñamos varias tareas para probar esta clase de habilidades. Primero, evaluamos la capacidad de GPT-3 para realizar operaciones aritméticas. En segundo lugar, creamos varias tareas que implican reorganizar o descifrar las letras en una palabra, tareas que es poco probable que se hayan visto exactamente durante el entrenamiento. En tercer lugar, probamos la capacidad de GPT-3 para resolver problemas de analogía de estilo SAT de pocos disparos. Finalmente, probamos GPT-3 en varias tareas cualitativas, incluido el uso de palabras nuevas en una oración, la corrección de la gramática inglesa y la generación de artículos de noticias. Lanzaremos los conjuntos de datos sintéticos con la esperanza de estimular un mayor estudio del comportamiento de los modelos de lenguaje durante el tiempo de prueba.

3.9.1 Aritmética Para probar la capacidad de GPT-3 de realizar operaciones aritméticas simples sin entrenamiento específico de la tarea, desarrollamos una pequeña batería de 10 pruebas que implican preguntarle a GPT-3 un problema aritmético simple en lenguaje natural: • Suma de 2 dígitos (2D +) - El Se le pide al modelo que agregue dos enteros muestreados de manera uniforme a partir de [0,100), redactados en forma de una pregunta, por ej. “P: ¿Qué es 48 más 76? A: 124. ”• Sustracción de 2 dígitos (2D -): se le pide al modelo que reste dos enteros muestreados uniformemente de [0,100); La respuesta puede ser negativa. Ejemplo: “P: ¿Qué es 34 menos 53? A: -19 ". • Suma de 3 dígitos (3D +): igual que la suma de 2 dígitos, excepto que los números se muestrean uniformemente de [0,1000)

FIGURA

Figura 3.10: Resultados de las 10 tareas aritméticas en la configuración de pocos disparos para modelos de diferentes tamaños. Hay un salto significativo del segundo modelo más grande (GPT-3 13B) al modelo más grande (GPT-3 175), siendo este último capaz de obtener una aritmética precisa de 2 dígitos, generalmente una aritmética precisa de 3 dígitos, y respuestas correctas a una fracción significativa del tiempo en aritmética de 4-5 dígitos, multiplicación de 2 dígitos y operaciones compuestas. Los resultados de un disparo y cero disparos se muestran en el apéndice

Resta de 3 dígitos (3D -): igual que la resta de 2 dígitos, excepto que los números se muestrean uniformemente desde [0,1000]. • Suma de 4 dígitos (4D +): igual que la suma de 3 dígitos, excepto que se muestrea de manera uniforme desde [0,10000). • Sustracción de 4 dígitos (4D -): igual que la sustracción de 3 dígitos, excepto una muestra uniforme de [0,10000). • Suma de 5 dígitos (5D +): igual que la suma de 3 dígitos, excepto una muestra uniforme de [0,100000). Resta de 5 dígitos (5D -): igual que la resta de 3 dígitos, excepto que se muestrea de manera uniforme a partir de [0,100000]. • Multiplicación de 2 dígitos (2Dx): se le pide al modelo que multiplique dos enteros muestreados de manera uniforme a partir de [0,100), p. Ej. “P: ¿Qué es 24 veces 42? A: 1008 ". • Compuesto de un dígito (1DC): se le pide al modelo que realice una operación compuesta en tres números de 1 dígito, con paréntesis alrededor de los dos últimos. Por ejemplo, “P: ¿Qué es 6+ (4 * 8)? A: 38 ". Los tres números de 1 dígito se seleccionan uniformemente en [0,10) y las operaciones se seleccionan uniformemente de {+, -, *}

En las 10 tareas, el modelo debe generar la respuesta correcta exactamente. Para cada tarea generamos un conjunto de datos de 2,000 instancias aleatorias de la tarea y evaluamos todos los modelos en esas instancias. Primero evaluamos GPT-3 en la configuración de pocos disparos, cuyos resultados se muestran en la Figura 3.10. En suma y resta, GPT-3 muestra una gran competencia cuando el número de dígitos es pequeño, logrando una precisión del 100% en la suma de 2 dígitos, 98.9% en la resta de 2 dígitos, 80.2% en la suma de 3 dígitos y 94.2% en la resta de 3 dígitos . El rendimiento disminuye a medida que aumenta el número de dígitos, pero GPT-3 aún logra un 25-26% de precisión en operaciones de cuatro dígitos y un 9-10% de precisión en operaciones de cinco dígitos, lo que sugiere al menos cierta capacidad de generalizar a un mayor número de dígitos. GPT-3 también logra 29.2% de precisión en la multiplicación de 2 dígitos, una operación especialmente computacionalmente intensiva. Finalmente, GPT-3 logra una precisión del 21.3% en operaciones combinadas de un solo dígito (por ejemplo, 9 * (7 + 5)), lo que sugiere que tiene algo de robustez más allá de las operaciones simples. Como la Figura 3.10 deja en claro, los modelos pequeños funcionan mal en todos de estas tareas, incluso el modelo de 13 mil millones de parámetros (el segundo más grande después de los 175 mil millones de GPT-3 completos) puede resolver sumas y restas de 2 dígitos solo la mitad del tiempo, y otras operaciones menos del 10% del tiempo. el rendimiento de los disparos está algo degradado en relación con el rendimiento de pocos disparos, lo que sugiere que la adaptación a la tarea (o al menos el reconocimiento de la tarea) es importante para realizar estos cálculos correctamente. Sin embargo, el rendimiento de un disparo sigue siendo bastante fuerte e incluso rendimiento de tiro cero del GPT-3 completo significativamente

TABLA

Tabla 3.9: Resultados en tareas aritméticas básicas para GPT-3 175B. {2,3,4,5} D {+, -} es una suma o resta de 2, 3, 4 y 5 dígitos, 2Dx es una multiplicación de 2 dígitos. 1DC es operaciones compuestas de 1 dígito. Los resultados se vuelven progresivamente más fuertes moviéndose desde el ajuste de disparo cero a uno a pocos disparos, pero incluso el disparo cero muestra habilidades aritméticas significativas.

TABLA

Tabla 3.10: Rendimiento GPT-3 175B en varias tareas de descifrado de palabras y manipulación de palabras, en configuraciones de cero, uno y pocos disparos. CL es "letras de ciclo en la palabra", A1 son anagramas de la primera y la última letra, A2 son anagramas de todas menos las dos primeras y últimas letras, RI es "inserción aleatoria en la palabra", RW son "palabras invertidas".

supera el aprendizaje de pocos disparos para todos los modelos más pequeños. Las tres configuraciones para el GPT-3 completo se muestran en la Tabla 3.9, y la escala de capacidad del modelo para las tres configuraciones se muestra en el Apéndice H. Para verificar si el modelo simplemente está memorizando problemas aritméticos específicos, tomamos los problemas aritméticos de 3 dígitos en nuestro conjunto de pruebas y los buscamos en nuestros datos de entrenamiento en las formas "<NUM1> + <NUM2> =" y "<NUM1> plus <NUM2>". De 2,000 problemas de suma encontramos solo 17 coincidencias (0.8%) y de 2,000 problemas de resta encontramos solo 2 coincidencias (0.1%), lo que sugiere que solo una fracción trivial de las respuestas correctas podría haber sido memorizada. Además, la inspección de respuestas incorrectas revela que el modelo a menudo comete errores como no llevar un "1", lo que sugiere que en realidad está intentando realizar el cálculo relevante en lugar de recordar una tabla. En general, GPT-3 muestra un dominio razonable en aritmética moderadamente compleja en Configuración de pocos disparos, un disparo e incluso cero. 3.9.2 Tareas de codificación y manipulación de palabras Para probar la capacidad de GPT-3 de aprender nuevas manipulaciones simbólicas a partir de algunos ejemplos, diseñamos una pequeña batería de 5 tareas de "manipulación de caracteres". Cada tarea implica darle al modelo una palabra distorsionada por alguna combinación de descifrado, adición o eliminación de caracteres, y pedirle que recupere la palabra original. Las 5 tareas son: • Ciclo de letras en la palabra (CL): se le da al modelo una palabra con sus letras en un ciclo, luego el símbolo "=" y se espera que genere la palabra original. Por ejemplo, se le puede dar "lyinevitab" y debería aparecer "inevitablemente". • Anagramas de todos menos el primer y el último carácter (A1): al modelo se le asigna una palabra donde cada letra, excepto la primera y la última, se ha mezclado aleatoriamente, y debe dar salida a la palabra original. Ejemplo: criroptuon = corrupción. • Anagramas de todos menos el primero y los últimos 2 caracteres (A2): al modelo se le asigna una palabra donde cada letra, excepto los primeros 2 y los últimos 2, se ha aleatorizado y debe recuperar la palabra original. Ejemplo: opoepnnt → oponente. • Inserción aleatoria en la palabra (RI): se inserta una puntuación aleatoria o un espacio entre cada letra de una palabra, y el modelo debe generar la palabra original. Ejemplo: su! C / c! Es si / o / n = sucesión. • Palabras invertidas (RW): al modelo se le asigna una palabra deletreada al revés y debe mostrar la palabra original. Ejemplo: stcejbo → objetos. Para cada tarea generar 10,000 ejemplos, que elegimos para ser las 10,000 palabras más frecuentes según lo medido por [Nor09] de longitud de más de 4 caracteres y menos de 15 caracteres. Los resultados de pocas tomas se muestran en la Figura 3.11. El rendimiento de la tarea tiende a crecer sin problemas con el tamaño del modelo, con el modelo completo GPT-3 logrando un 66.9% al eliminar

FIGURA

Figura 3.11: rendimiento de pocas tomas en las tareas de codificación de cinco palabras para diferentes tamaños de modelo. En general, hay una mejora suave con el tamaño del modelo, aunque la tarea de inserción aleatoria muestra una pendiente ascendente de mejora con el modelo 175B que resuelve la tarea la mayoría de las veces. La escala del rendimiento de un disparo y disparo cero se muestra en el apéndice. Todas las tareas se realizan con K = 100.

inserciones aleatorias, 38.6% en letras de ciclo, 40.2% en la tarea de anagrama más fácil y 15.1% en la tarea de anagrama más difícil (donde solo se mantienen fijas la primera y la última letra). Ninguno de los modelos puede revertir las letras en una palabra. En la configuración de un disparo, el rendimiento es significativamente más débil (se reduce a la mitad o más), y en la configuración de disparo cero, el modelo rara vez puede realizar alguna de las tareas (Tabla 3.10) . Esto sugiere que el modelo realmente parece aprender estas tareas en el momento de la prueba, ya que el modelo no puede realizar el disparo cero y su naturaleza artificial hace que sea poco probable que aparezca en los datos previos al entrenamiento (aunque no podemos confirmar esto con certeza). cuantifique aún más el rendimiento trazando "curvas de aprendizaje en contexto", que muestran el rendimiento de la tarea como una función del número de ejemplos en contexto. Mostramos curvas de aprendizaje en contexto para la inserción de símbolos Taskin Figura 1.2. Podemos ver que los modelos más grandes pueden hacer un uso cada vez más eficaz de la información en contexto, incluidos los ejemplos de tareas y las descripciones de tareas en lenguaje natural. Finalmente, vale la pena agregar que resolver estas tareas requiere manipulaciones a nivel de caracteres, mientras que nuestra codificación BPE funciona en fracciones significativas de una palabra (en promedio ∼0.7 palabras por token), por lo que desde la perspectiva del LM el éxito de estas tareas implica no solo manipular los tokens BPE sino comprender y separar su subestructura. Además, CL, A1 y A2 no son biyectivos (es decir, la palabra no codificada no es una función determinista de la palabra codificada), lo que requiere que el modelo realice alguna búsqueda para encontrar la codificación correcta. Por lo tanto, las habilidades involucradas parecen requerir una comparación y cálculo de patrones no triviales. 3.9.3 Analogías SAT Para probar GPT-3 en otra tarea que es algo inusual en relación con la distribución típica de texto, recolectamos un conjunto de 374 problemas de "analogía SAT" [TLBS03]. Las analogías son un estilo de preguntas de opción múltiple que constituyeron una sección del examen de ingreso a la universidad SAT antes de 2005. Un ejemplo típico es "audaz es la audacia como (a) santurioso es hipocresía, (b) anónimo es identidad, (c) remordimiento es cometer un error, (d) perjudicial es resultar, (e) impresionable es totemptation ”. Se espera que el alumno elija cuál de los cinco pares de palabras tiene la misma relación que el par de palabras original; En este ejemplo, la respuesta es "santurrona es hipocresía". En esta tarea, GPT-3 alcanza el 65.2% en la configuración de pocos disparos, el 59.1% en la configuración de un disparo y el 53.7% en la configuración de disparo cero, mientras que el puntaje promedio entre los solicitantes universitarios fue del 57% [TL05] (rendimiento de conjeturas aleatorias 20%). Como se muestra en la Figura 3.12, los resultados mejoran con la escala, con el modelo completo de 175 mil millones mejorando en más del 10% en comparación con el modelo de parámetro de 13 mil millones.

FIGURA

Figura 3.12: Rendimiento de cero, uno y pocos disparos en tareas de analogía SAT, para diferentes tamaños de modelo. El modelo más grande logra un 65% de precisión en la configuración de pocos disparos, y también demuestra ganancias significativas para el aprendizaje en contexto que no están presentes en modelos más pequeños

3.9.4 Generación de artículos de noticias El trabajo anterior sobre modelos de lenguaje generativo probó cualitativamente su capacidad de generar "artículos de noticias" sintéticos mediante el muestreo condicional del modelo dado un mensaje escrito por humanos que consiste en una primera oración plausible para una historia [RWC + 19]. En relación con [RWC + 19], el conjunto de datos utilizado para entrenar GPT-3 tiene mucho menos peso hacia los artículos de noticias, por lo que tratar de generar artículos de noticias a través de muestras incondicionales sin procesar es menos efectivo; por ejemplo, GPT-3 a menudo interpreta la primera oración propuesta de un "Artículo de noticias" como un tweet y luego publica respuestas sintéticas o tweets de seguimiento. Para resolver este problema, empleamos las habilidades de aprendizaje de tiro corto de GPT-3 al proporcionar tres artículos de noticias anteriores en el contexto del modelo para condicionarlo. Con el título y el subtítulo de un próximo artículo propuesto, el modelo puede generar de manera confiable artículos cortos en el género de "noticias". Para medir la calidad de la generación de artículos de noticias de GPT-3 (que creemos que probablemente esté correlacionado con la generación de muestras condicional calidad en general), decidimos medir la capacidad humana para distinguir los artículos generados por GPT-3 de los reales. Un trabajo similar ha sido llevado a cabo por Kreps et al. [KMB20] y Zellers et al. [ZHR + 19]. Los modelos de lenguaje generativo están entrenados para coincidir con la distribución del contenido generado por los humanos, por lo que la (in) capacidad de los humanos para distinguir los dos es una medida de calidad potencialmente importante.3 Para ver qué tan bien los humanos pueden detectar el texto generado por el modelo, seleccionamos arbitrariamente 25 títulos de los artículos y subtítulos del sitio web newser.com (longitud media: 215 palabras). Luego generamos terminaciones de estos títulos y subtítulos a partir de cuatro modelos de idiomas que varían en tamaño desde parámetros de 125M a 175B (GPT-3) (longitud media: 200 palabras). Para cada modelo, presentamos a alrededor de 80 participantes en los EE. UU. Un cuestionario que consta de estos títulos y subtítulos reales, seguido por el artículo escrito por humanos o el artículo generado por el modelo4. Se pidió a los participantes que seleccionaran si el artículo fue "muy probablemente escrito por un humano", "más probablemente escrito por un humano", "no sé", "más probablemente escrito por una máquina" o "muy probablemente escrito por una máquina". ". Los artículos que seleccionamos no estaban en los datos de entrenamiento de los modelos y los resultados del modelo fueron formateados y seleccionados programáticamente para evitar la recolección de cerezas en humanos. Todos los modelos usaron el mismo contexto para condicionar los resultados y fueron entrenados previamente con el mismo tamaño de contexto y los mismos títulos y subtítulos de artículos se usaron como indicaciones para cada modelo. Sin embargo, también realizamos un experimento para controlar el esfuerzo y la atención de los participantes que siguió el mismo formato pero implicado intencionalmente artículos generados por modelos incorrectos. Esto se hizo generando artículos a partir de un "modelo de control": un modelo de parámetros de 160M sin contexto y una aleatoriedad de salida incrementada.

Esta tarea también es relevante para el uso indebido potencial de los modelos de lenguaje discutidos en la Sección 6.1.4. Queríamos identificar qué tan buena es una persona promedio en Internet para detectar los resultados del modelo de lenguaje, por lo que nos centramos en los participantes provenientes de la población general de los Estados Unidos. Consulte el Apéndice E para más detalles.

TABLA

Tabla 3.11: Precisión humana para identificar si los artículos de noticias breves (∼200 palabras) se generan por modelo. Encontramos que la precisión humana (medida por la proporción de asignaciones correctas a asignaciones no neutrales) varía del 86% en el modelo de control al 52% en GPT-3 175B. Esta tabla compara la precisión media entre cinco modelos diferentes y muestra los resultados de una prueba T de dos muestras para la diferencia en la precisión media entre cada modelo y el modelo de control (un modelo pequeño GPT-3 incondicional con aleatoriedad de salida incrementada).

La precisión humana promedio (la proporción de asignaciones correctas a asignaciones no neutrales por participante) para detectar que los artículos intencionalmente malos fueron generados por el modelo fue de ~ 86%, donde 50% es un rendimiento de nivel de oportunidad. Por el contrario, la precisión humana promedio en la detección de artículos producidos por el modelo de parámetro 175B fue apenas superior al azar en un 52% (ver Tabla 3.11) .5 Las capacidades humanas para detectar texto generado por el modelo parecen disminuir a medida que aumenta el tamaño del modelo: parece haber un tendencia hacia la precisión del azar con el tamaño del modelo, y la detección humana de GPT-3 está cerca del azar.6 Esto es cierto a pesar del hecho de que los participantes pasan más tiempo en cada salida a medida que aumenta el tamaño del modelo (ver Apéndice E). Ejemplos de artículos sintéticos de GPT -3 se dan en las Figuras 3.14 y 3.15.7. Gran parte del texto es, como lo indican las evaluaciones, difícil de distinguir para los humanos del contenido humano auténtico. Las imprecisiones fácticas pueden ser una indicación de que un artículo es un modelo generado ya que, a diferencia de los autores humanos, los modelos no tienen acceso a los hechos específicos a los que se refieren los títulos de los artículos o cuando se escribió el artículo. Otros indicadores incluyen repetición, no secuencia y frases inusuales, aunque a menudo son lo suficientemente sutiles como para que no se noten. El trabajo relacionado en la detección de modelos de lenguaje por Ippolito et al. [IDCBE19] indica que los discriminadores automáticos como GR O V E R [ZHR + 19] y GLTR [GSR19] pueden tener mayor éxito en la detección de texto generado por modelos que los evaluadores humanos. La detección automática de estos modelos puede ser un área prometedora para futuras investigaciones. Eppolito et al. [IDCBE19] también tenga en cuenta que la precisión humana en la detección de texto generado por el modelo aumenta a medida que los humanos observan más fichas. Para hacer una investigación preliminar de cuán buenos son los humanos para detectar artículos de noticias más largos generados por GPT-3 175B, seleccionamos 12 artículos de noticias mundiales de Reuters con una longitud promedio de 569 palabras y generamos complementos de estos artículos de GPT-3 con una longitud promedio de 498 palabras (298 palabras más largas que nuestros experimentos iniciales). Siguiendo la metodología anterior, realizamos dos experimentos, cada uno con alrededor de 80 participantes en los EE. UU., Para comparar las capacidades humanas para detectar los artículos generados por GPT-3 y un modelo de control. Encontramos que la precisión humana significa detectar los artículos más largos intencionalmente malos de el modelo de control fue de ~ 88%, mientras que la precisión humana promedio en la detección de los artículos más largos producidos por GPT-3 175B todavía tenía una probabilidad mínima de ~ 52% (ver Tabla 3.12). Esto indica que, para artículos de noticias que tienen alrededor de 500 palabras, GPT-3 continúa produciendo artículos que los humanos encuentran difíciles de distinguir de los artículos de noticias escritos por humanos. 3.9.5 Aprendizaje y uso de palabras nuevas Una tarea estudiada en lingüística del desarrollo [CB78] es la capacidad de aprender y utilizar nuevas palabras, por ejemplo, usar una palabra en una oración después de verla definida solo una vez, o por el contrario inferir el significado de una palabra de un solo uso. Aquí probamos cualitativamente la capacidad de GPT-3 para hacer lo primero. Específicamente, le damos a GPT-3 la definición de una palabra inexistente, como “Gigamuru”, y luego le pedimos que la use en una oración. Proporcionamos de uno a cinco ejemplos anteriores de un 5 (separado) Usamos una prueba T de Student de dos muestras para probar la diferencia significativa entre las medias de las precisiones de los participantes de cada modelo y el modelo de control e informar la diferencia normalizada en las medias (como el estadístico t) y el valor p.6 Si un modelo produce consistentemente textos que son más impresionantes que los artículos humanos, es posible que el rendimiento humano en esta tarea caiga por debajo del 50%. De hecho, muchos participantes individuales obtuvieron puntajes inferiores al 50% en esta tarea.7 Se pueden encontrar muestras adicionales que no son noticias en el Apéndice F.

FIGURA

Figura 3.13: La capacidad de las personas para identificar si los artículos de noticias son generados por modelos (medidos por la proporción de asignaciones correctas a asignaciones no neutrales) disminuye a medida que aumenta el tamaño del modelo. La precisión en las salidas en el modelo de control deliberadamente malo (un modelo pequeño GPT-3 incondicionado con mayor aleatoriedad de salida) se indica con la línea discontinua en la parte superior, y la probabilidad aleatoria (50%) se indica con la línea discontinua en la parte inferior . La línea de mejor ajuste es una ley de poder con intervalos de confianza del 95%

TABLA

Tabla 3.12: La capacidad de las personas para identificar si los artículos de 500 palabras son generados por el modelo (medido por la proporción de asignaciones correctas a asignaciones no neutrales) fue del 88% en el modelo de control y del 52% en GPT-3 175B. Esta tabla muestra los resultados de una prueba T de dos muestras para la diferencia en la precisión media entre GPT-3 175B y el modelo de control (un modelo pequeño GPT-3 incondicional con aleatoriedad de salida incrementada).

Título: Metodistas Unidos acuerdan una división histórica Subtítulo: Aquellos que se oponen al matrimonio homosexual formarán su propia denominación Artículo: Después de dos días de intenso debate, la Iglesia Metodista Unida ha acordado una división histórica, una que se espera que termine en la creación de una nueva denominación, uno que será "teológica y socialmente conservador", según The Washington Post. La mayoría de los delegados que asistieron a la Conferencia General anual de la iglesia en mayo votaron para fortalecer una prohibición de la ordenación del clero LGBTQ y para escribir nuevas reglas que "disciplinarán" al clero que oficia en bodas del mismo sexo. Pero aquellos que se opusieron a estas medidas tienen un nuevo plan: dicen que formarán una denominación separada para 2020, llamando a su iglesia la denominación metodista cristiana. El Post señala que la denominación, que cuenta con 12,5 millones de miembros, fue a principios del siglo XX la "denominación protestante más grande en los Estados Unidos ", pero que se ha reducido en las últimas décadas. La nueva división será la segunda en la historia de la iglesia. El primero ocurrió en 1968, cuando aproximadamente el 10 por ciento de la denominación se fue para formar la Iglesia Evangélica Unida de Hermanos. El Post señala que la división propuesta "llega en un momento crítico para la iglesia, que ha estado perdiendo miembros durante años", que ha sido "empujada al borde de un cisma sobre el papel de las personas LGBTQ en la iglesia". El matrimonio gay no es el único problema que ha dividido a la iglesia. En 2016, la denominación se dividió sobre la ordenación del clero transgénero, con la votación de la conferencia regional del Pacífico Norte para prohibir que sirvieran a la alergia, y la votación de la conferencia regional del Pacífico Sur para permitirlos.

Figura 3.14: El GPT-3 generó un artículo de noticias en el que los humanos tenían la mayor dificultad para distinguirlo de un artículo escrito por humanos (precisión: 12%).

Título: La promesa de Tux de Star dibuja el sarcasmo de Megyn Kelly Subtítulo: Joaquin Phoenix se comprometió a no cambiar para cada evento de premios Artículo: Hace un año, Joaquin Phoenix fue noticia cuando apareció en la alfombra roja de los Globos de Oro con un esmoquin con una bolsa de papel sobre su cabeza que decía "Soy un cambiaformas. No puedo cambiar el mundo. Solo puedo cambiarme a mí mismo". Era una promesa de no cambiar para encajar en el molde de Hollywood: "Creo que es algo realmente especial, no cambiarse a sí mismo. Creo que es algo realmente especial decir: 'Esto es lo que hay dentro de mí, estoy orgulloso de y no me avergonzaré por la forma en que alguien más piensa que debería estarlo ". Oscars, y Phoenix está en eso otra vez. Pero esta vez, su publicista dice que usará un esmoquin, pase lo que pase. Megyn Kelly no estaba impresionada, y ella lo dejó tenerlo en The Tonight Show. "Sabes, siento que siento que podrías haber usado el esmoquin ", dice ella." Pero estás diciendo que cambias de forma. No sé si puedes cambiar tu esmoquin, pero puedes cambiarlo. tu mente. Puedes cambiar de opinión. Puedes cambiar de opinión ". Phoenix dice que sí, pero no se pegó." Pensé: 'Bueno, voy a ponerme un esmoquin para esto'. Y luego pensé: "No quiero usar un esmoquin para esto". Kelly continúa alentándolo a que cambie de opinión nuevamente, pero Phoenix dice que es demasiado tarde: "Estoy comprometido a usar esto".

Figura 3.15: El GPT-3 generó un artículo de noticias que los humanos encontraron más fácil de distinguir de un artículo escrito humano (precisión: 61%).

Un "whatpu" es un animal pequeño y peludo nativo de Tanzania. Un ejemplo de una oración que usa la palabra whatpu es: Estábamos viajando en África y vimos estos whatpus muy lindos. Hacer un "farduddle" significa saltar arriba y abajo muy rápido. Un ejemplo de una oración que usa la palabra farduddle es: Un día, cuando estaba jugando al juego con mi hermana pequeña, se emocionó mucho y comenzó a hacer estos farduddles locos. Un "yalubalu" es un tipo de verdura que parece una gran calabaza. Un ejemplo de una oración que usa la palabra yalubalu es: estaba en un viaje a África y probé este vegetal yalubalu que se cultivaba en un jardín aquí. Estaba delicioso. Un "Burringo" es un automóvil con una aceleración muy rápida. Un ejemplo de una oración que usa la palabra Burringo es: En nuestro garaje tenemos un Burringo que mi padre conduce al trabajo todos los días. Un "Gigamuru" es un tipo de instrumento musical japonés. Un ejemplo de una oración que usa la palabra Gigamuru es: Tengo un Gigamuru que mi tío me regaló. Me encanta jugarlo en casa. "Gritar" algo es lanzarle una espada. Un ejemplo de una oración que usa las palabras chirrido es: nos gritamos unos a otros durante varios minutos y luego salimos a comer helado.

Figura 3.16: Terminaciones representativas de GPT-3 para la tarea de pocos disparos de usar una nueva palabra en una oración. Las letras en negrita son las terminaciones de GPT-3, el texto sin formato son indicaciones humanas. En el primer ejemplo, tanto el aviso como la finalización son proporcionados por un humano; esto luego sirve como condicionamiento para ejemplos posteriores en los que GPT-3 recibe sucesivos mensajes adicionales y proporciona las terminaciones. No se proporciona nada específico de la tarea a GPT-3 aparte del condicionamiento que se muestra aquí.

la palabra inexistente se define y se usa en una oración, por lo que la tarea es de pocos disparos en términos de ejemplos anteriores de la tarea amplia y de un solo golpe en términos de la palabra específica. La tabla 3.16 muestra los 6 ejemplos que generamos; todas las definiciones fueron generadas por humanos, y la primera respuesta fue generada por humanos como condicionamiento, mientras que las respuestas posteriores fueron generadas por GPT-3. Estos ejemplos se generaron continuamente en una sola sesión y no omitimos ni repetidamente intentamos cualquier indicación. En todos los casos, la oración generada parece ser un uso correcto o al menos plausible de la palabra. En la frase final, el modelo genera una conjugación plausible para la palabra "screeg" (es decir, "screeghed"), aunque el uso de la palabra es ligeramente incómodo ("screeghed uno al otro") a pesar de ser plausible en el sentido de que podría describir una palabra de juguete lucha. En general, GPT-3 parece ser al menos competente en la tarea de usar palabras nuevas en una oración. 3.9.6 Corrección de la gramática del inglés Otra tarea adecuada para el aprendizaje de pocos disparos es corregir la gramática del inglés. Probamos esto con GPT-3 en la configuración de pocos disparos dando indicaciones de la forma "Pobre entrada en inglés: <sentencia> \ n Buena salida en inglés: <sentencia>". Le damos a GPT-3 una corrección generada por humanos y luego le pedimos que corrija 5 más (de nuevo sin ninguna omisión ni repetición). Los resultados se muestran en la Figura 3.17.4 Medición y prevención de la memorización de los puntos de referencia Dado que nuestro conjunto de datos de capacitación se obtiene de Internet, es posible que nuestro modelo haya sido entrenado en algunos de nuestros conjuntos de pruebas de referencia. Detectar con precisión la contaminación de prueba de los conjuntos de datos a escala de Internet es una nueva área de investigación sin las mejores prácticas establecidas. Si bien es una práctica común entrenar modelos grandes sin investigar la contaminación, dada la creciente escala de los conjuntos de datos de preentrenamiento, creemos que este problema es cada vez más importante de atender. Esta preocupación no es solo hipotética. Uno de los primeros documentos en entrenar un modelo de lenguaje sobre datos de rastreo común [TL18] detectó y eliminó un documento de entrenamiento que se superponía con uno de sus conjuntos de datos de evaluación. Otros trabajos como GPT-2 [RWC + 19] también realizaron análisis de superposición post-hoc. Su estudio fue relativamente alentador, descubriendo que

Insumo pobre en inglés: comí las bayas moradas. Buen resultado en inglés: comí las bayas moradas. Insumo pobre en inglés: Gracias por elegirme como su diseñador. Se lo agradeceré. Buen resultado en inglés: gracias por elegirme como su diseñador. Se lo agradezco. Poca aportación en inglés: los cambios mencionados se han realizado. o hice la alteración que solicitaste. o cambié las cosas que quería e hice las modificaciones. Buen resultado en inglés: se han realizado los cambios solicitados. o hice la alteración que solicitaste. o cambié las cosas que quería e hice las modificaciones. Entrada pobre en inglés: estaría más que feliz de trabajar con usted en otro proyecto. Buena salida en inglés: estaría más que feliz de trabajar con usted en otro proyecto. Ingreso en inglés: por favor bríndeme un breve resumen del diseño que está buscando y sería bueno si pudiera compartir algunos ejemplos o proyectos que hizo antes. Buen resultado en inglés: por favor bríndeme una breve descripción del diseño que 'buscando y eso sería bueno si pudieras compartir algunos ejemplos o proyectos que ha hecho antes. Poca entrada en inglés: el paciente falleció. Buena producción en inglés: el paciente murió. Poca entrada en inglés: creemos que Leslie nos quiere a nosotros mismos. Buena salida en inglés: creemos que Leslie nos quiere. Poca entrada en inglés : Janet rompió a Bill en el dedo. Buena producción en inglés: Janet rompió el dedo de Bill. Mala entrada en inglés: Mary arregló para que, en St. Louis, John alquilara una casa barata. Buena producción en inglés: Mary arregló para que John alquilara una casa en S t.          Louis.Puerta entrada en inglés: hoy he ido a la tienda a comprar algunas botellas de agua. Buena producción en inglés: hoy fui a la tienda a comprar algunas botellas de agua. Poca entrada en inglés: he tratado de golpear la pelota con bate, pero mi swing es fall. Buena salida en inglés: intenté golpear la pelota con el bate, pero mi swing falló

Figura 3.17: Finalizaciones representativas de GPT-3 para la tarea de pocos disparos de corregir la gramática inglesa. Las terminaciones de Boldfaceis GPT-3, texto plano son mensajes humanos. En los primeros ejemplos, tanto el aviso como la finalización son proporcionados por un humano; esto sirve como condicionamiento para ejemplos posteriores en los que GPT-3 recibe indicaciones adicionales sucesivas y proporciona las finalizaciones. GPT-3 no proporciona nada específico de la tarea, aparte de los primeros ejemplos, como el condicionamiento y el encuadre “Pobre entrada de inglés / Buena salida de inglés”. Notamos que la distinción entre inglés "pobre" y "bueno" (y los términos en sí mismos) es compleja, contextual y controvertida. En el ejemplo que menciona el alquiler de una casa, las suposiciones que hace el modelo sobre lo que es "bueno" pueden incluso llevarlo a cometer errores (aquí, el modelo no solo ajusta la gramática, sino que también elimina la palabra "barato" de una manera que altera el significado )

FIGURA

Figura 4.1: Curvas de entrenamiento GPT-3 Medimos el rendimiento del modelo durante el entrenamiento en una división de validación deduplicada de nuestra distribución de entrenamiento. Aunque hay una brecha entre el entrenamiento y el rendimiento de validación, la brecha crece solo mínimamente con el tamaño del modelo y el tiempo de entrenamiento, lo que sugiere que la mayor parte de la brecha proviene de una diferencia de dificultad en lugar de sobreajuste.

Aunque los modelos tuvieron un rendimiento moderadamente mejor en los datos que se superponen entre el entrenamiento y las pruebas, esto no tuvo un impacto significativo en los resultados informados debido a la pequeña fracción de datos que estaba contaminada (a menudo solo un pequeño porcentaje). GPT-3 opera en un régimen algo diferente. Por un lado, el conjunto de datos y el tamaño del modelo son aproximadamente dos órdenes de magnitud más grandes que los utilizados para GPT-2 e incluyen una gran cantidad de rastreo común, lo que crea un mayor potencial de contaminación y memorización. Por otro lado, precisamente debido a la gran cantidad de datos, incluso GPT-3 175B no sobrepasa su conjunto de entrenamiento en una cantidad significativa, medida en relación con un conjunto de validación extendido con el que fue duplicada (Figura 4.1). Por lo tanto, esperamos que la contaminación sea frecuente, pero que sus efectos no sean tan grandes como se temía. Inicialmente intentamos abordar el problema de la contaminación buscando e intentando eliminar cualquier superposición entre nuestros datos de capacitación y el desarrollo y las pruebas. conjuntos de todos los puntos de referencia estudiados en este documento. Desafortunadamente, abug resultó en una eliminación parcial de todas las superposiciones detectadas de los datos de entrenamiento. Debido al costo de la capacitación, no era factible volver a capacitar al modelo. Para abordar esto, investigamos en detalle cómo la superposición detectada restante impacta los resultados. Para cada punto de referencia, producimos una versión 'limpia' que elimina todos los ejemplos potencialmente filtrados, definidos aproximadamente como ejemplos que tienen una superposición de 13 gramos con cualquier cosa en el conjunto de entrenamiento previo ( o que se superponen con todo el ejemplo cuando es más corto que 13 gramos). El objetivo es marcar de manera muy conservadora cualquier cosa que pueda ser potencialmente contaminante, a fin de producir un subconjunto limpio que esté libre de contaminación con alta confianza. El procedimiento exacto se detalla en el Apéndice C. Luego evaluamos GPT-3 en estos puntos de referencia limpios y los comparamos con el puntaje original. Si la puntuación en el subconjunto limpio es similar a la puntuación en todo el conjunto de datos, esto sugiere que la contaminación, incluso si está presente, no tiene un efecto significativo en los resultados informados. Si la puntuación en el subconjunto limpio es menor, esto sugiere que la contaminación puede estar infestando los resultados. Los resultados se resumen en la Figura 4.2. Aunque la contaminación potencial es a menudo alta (con un índice de puntos de referencia superior al 50%), en la mayoría de los casos, el rendimiento cambia solo de manera insignificante, y no vemos evidencia de que el nivel de contaminación y la diferencia de rendimiento estén correlacionados. Concluimos que nuestros métodos conservadores sobrestiman considerablemente la contaminación o que la contaminación tiene poco efecto en el rendimiento. A continuación, revisamos con más detalle los pocos casos específicos en los que (1) el modelo funciona significativamente peor en la versión limpia, o (2) la contaminación potencial es muy alto, lo que hace que medir el rendimiento sea difícil de diferenciar. Nuestro análisis marcó seis grupos de puntos de referencia para futuras investigaciones: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, tareas de modelado de idiomas (tareas de Wikitext, 1BW) y alemán a Ingles

FIGURA

Figura 4.2: Análisis de referencia de contaminación Hemos construido versiones limpias de cada uno de nuestros puntos de referencia para verificar la posible contaminación en nuestro conjunto de capacitación. El eje x es un límite inferior conservador para la cantidad de datos que se sabe con alta confianza para estar limpio, y el eje y muestra la diferencia en el rendimiento cuando se evalúa solo en el subconjunto limpio verificado. El rendimiento en la mayoría de los puntos de referencia cambió de manera insignificante, pero algunos se marcaron para una revisión adicional. En la inspección encontramos evidencia de contaminación de los resultados de PIQA y Winograd, y marcamos los resultados correspondientes en la Sección 3 con un asterisco. No encontramos evidencia de que otros puntos de referencia se vean afectados.

Traducción. Dado que nuestro análisis de superposición está diseñado para ser extremadamente conservador, esperamos que produzca algunos falsos positivos. A continuación resumimos los resultados para cada grupo de tareas: • Comprensión de lectura: Nuestro análisis inicial marcó> 90% de los ejemplos de tareas de QuAC, SQuAD2 y DROP como potencialmente contaminados, tan grande que incluso medir el diferencial en un subconjunto limpio fue difícil. Sin embargo, en la inspección manual, encontramos que por cada solapamiento que inspeccionamos, en los 3 conjuntos de datos, el texto fuente estaba presente en nuestros datos de entrenamiento, pero los pares de preguntas / respuestas no estaban, lo que significa que el modelo solo obtiene información de fondo y no puede memorizar la respuesta a un determinado • Traducción al alemán: encontramos que el 25% de los ejemplos en el conjunto de prueba alemán-inglés WMT16 estaban comercializados como potencialmente contaminados, con un tamaño de efecto total asociado de 1-2 BLEU. Tras la inspección, ninguno de los ejemplos señalados contiene oraciones pareadas que se asemejan a los datos de entrenamiento de NMT y las colisiones fueron coincidencias monolingües, en su mayoría, de fragmentos de eventos discutidos en las noticias. Debido a la corta duración de estas tareas, utilizamos 2 gramos para filtrar (ignorando la puntuación). Después de inspeccionar los flaggedoverlaps, descubrimos que no eran típicamente casos de reversiones reales o descifrados en el conjunto de entrenamiento, sino más bien palíndromos o descifrados triviales, por ejemplo, "kayak = kayak". La cantidad de superposición fue pequeña, pero eliminar las tareas triviales condujo a un aumento de la dificultad y, por lo tanto, a una señal espuria. En relación con esto, la tarea de inserción de símbolos muestra una gran superposición, pero no tiene ningún efecto en el rendimiento, esto se debe a que esa tarea implica eliminar caracteres que no son letras de una palabra, y el análisis de superposición en sí ignora dichos caracteres, lo que lleva a muchas coincidencias espurias. marcó el 29% de los ejemplos como contaminado y observó una disminución absoluta de 3 puntos porcentuales (disminución relativa del 4%) en el rendimiento en el subconjunto limpio. Aunque el conjunto de datos de prueba se lanzó después de que se creó nuestro conjunto de entrenamiento y sus etiquetas están ocultas, algunas de las páginas web utilizadas por los creadores de conjuntos de datos financiados por el público están contenidas en nuestro conjunto de entrenamiento. Encontramos una disminución similar en un modelo 25 veces más pequeño con mucha menos capacidad para memorizar, lo que nos lleva a sospechar que el cambio es probablemente más sesgo estadístico que la memorización; ejemplos que los trabajadores copiaron pueden ser simplemente más fáciles. Desafortunadamente, no podemos probar rigurosamente esta hipótesis. Por lo tanto, marcamos nuestros resultados de PIQA con un asterisco para denotar esta contaminación potencial. • Winograd: el análisis de superposición marcó el 45% de los ejemplos y encontró una disminución del 2.6% en el rendimiento en el subconjunto limpio. La inspección manual del punto de datos superpuestos mostró que 132 esquemas de Winograd estaban presentes en nuestro conjunto de entrenamiento, aunque se presentaron en un formato diferente al que presentamos la tarea al modelo. Aunque la disminución en el rendimiento es pequeña, marcamos nuestros resultados de Winograd en el Papel principal con anasterisk.

Modelado de idiomas: encontramos que los 4 puntos de referencia de modelado de idiomas de Wikipedia medidos en GPT-2, más el conjunto de datos de la Prueba del libro de los niños, están casi completamente contenidos en nuestros datos de capacitación. Como no podemos extraer de manera confiable un subconjunto limpio aquí, no informamos los resultados de estos conjuntos de datos, a pesar de que teníamos la intención de comenzar este trabajo. Observamos que Penn Tree Bank debido a su antigüedad no se vio afectado y, por lo tanto, se convirtió en nuestro principal referente de modelado de idiomas.

También inspeccionamos conjuntos de datos donde la contaminación era alta, pero el impacto en el rendimiento era cercano a cero, simplemente para verificar cuánta contaminación real existía. Estos parecían contener a menudo falsos positivos. No tenían contaminación real, o tenían contaminación que no revelaba la respuesta a la tarea. Una excepción notable fue LAMBADA, que parecía tener una contaminación genuina sustancial, pero el impacto en el rendimiento fue muy pequeño, con un puntaje de subconjunto limpio dentro del 0.5% del conjunto de datos completo. Además, estrictamente hablando, nuestro formato de relleno en blanco incluye la forma más simple de memorización. Sin embargo, dado que obtuvimos grandes ganancias en LAMBADA en este documento, la contaminación potencial se observa en la sección de resultados. Una limitación importante de nuestro análisis de contaminación es que no podemos estar seguros de que el subconjunto limpio se extraiga de la misma distribución como el conjunto de datos original. Es posible que la memorización aumente los resultados, pero al mismo tiempo se contrarresta con precisión por algún sesgo estadístico que hace que el subconjunto limpio sea más fácil. Sin embargo, la gran cantidad de cambios cercanos a cero sugiere que esto es poco probable, y tampoco observamos una diferencia notable en los cambios para los modelos pequeños, que es poco probable que se memoricen. En general, hemos hecho un gran esfuerzo para medir y documentar los efectos de los datos. contaminación, y para anotar o eliminar de inmediato los resultados problemáticos, dependiendo de la gravedad. Queda mucho trabajo por hacer para abordar este tema importante y sutil para el campo en general, tanto al diseñar puntos de referencia como al entrenar modelos. Para una explicación más detallada de nuestro análisis, remitimos al lector al Apéndice C.5 Limitaciones GPT-3 y nuestro análisis tiene varias limitaciones. A continuación describimos algunos de estos y sugerimos instrucciones para el trabajo futuro. Primero, a pesar de las fuertes mejoras cuantitativas y cualitativas de GPT-3, particularmente en comparación con su predecesor directo GPT-2, todavía tiene notables debilidades en la síntesis de texto y varias tareas de PNL. En la síntesis de texto, aunque la calidad general es alta, las muestras GPT-3 a veces se repiten semánticamente a nivel de documento, comienzan a perder coherencia en pasajes suficientemente largos, se contradicen y ocasionalmente contienen oraciones o párrafos no secuenciales. Lanzaremos una colección de 500 muestras incondicionales no curadas para ayudar a proporcionar una mejor idea de las limitaciones y fortalezas de GPT-3 en la síntesis de texto. Dentro del dominio de las tareas de lenguaje discreto, hemos notado informalmente que GPT-3 parece tener una dificultad especial con la "física del sentido común", a pesar de tener buenos resultados en algunos conjuntos de datos (como PIQA [BZB + 19]) que prueban este dominio. Específicamente, GPT-3 tiene dificultades con preguntas del tipo "Si pongo queso en el refrigerador, ¿se derretirá?". Cuantitativamente, el rendimiento de aprendizaje en contexto de GPT-3 tiene algunas brechas notables en nuestro conjunto de puntos de referencia, como se describe en la Sección 3, y en particular hace poco mejor que la posibilidad de evaluar una sola vez o incluso pocas veces en algunas tareas de "comparación", como determinar si dos palabras se usan de la misma manera en una oración, o si una oración implica otra (WIC y ANLI respectivamente), así como en un subconjunto de tareas de comprensión de lectura. Esto es especialmente sorprendente dado el fuerte rendimiento de pocos disparos de GPT-3 en muchas otras tareas. GPT-3 tiene varias limitaciones estructurales y algorítmicas, que podrían explicar algunos de los problemas anteriores. Nos enfocamos en explorar el comportamiento de aprendizaje en contexto en los modelos de lenguaje autorregresivo porque es sencillo, tanto muestrear como calcular probabilidades con esta clase de modelo. Como resultado, nuestros experimentos no incluyen ninguna arquitectura bidireccional de alarmas u otros objetivos de capacitación como la eliminación de ruido. Esta es una diferencia notable de gran parte de la literatura reciente, que ha documentado un rendimiento de ajuste fino mejorado al usar estos enfoques sobre modelos de lenguaje estándar [RSR + 19]. Por lo tanto, nuestra decisión de diseño tiene el costo de un rendimiento potencialmente peor en tareas que se benefician empíricamente de la bidireccionalidad. Esto puede incluir tareas para llenar el espacio en blanco, tareas que implican mirar hacia atrás y comparar dos piezas de contenido, o tareas que requieren volver a leer o considerar cuidadosamente un pasaje largo y generar una respuesta muy corta. Esta podría ser una posible explicación para el rendimiento rezagado de pocos disparos de GPT-3 en algunas de las tareas, como WIC (que implica comparar el uso de una palabra en dos oraciones), ANLI (que implica comparar dos oraciones para ver si una implica otro), y varias tareas de comprensión de lectura (por ejemplo, QuAC y RACE). También conjeturamos, en base a literatura anterior, que un modelo bidireccional grande sería más fuerte en el ajuste que GPT-3. Hacer un modelo bidireccional a la escala de GPT-3, y / o tratar de hacer que los modelos bidireccionales funcionen con pocos o cero aprendizajes, es una dirección prometedora para futuras investigaciones y podría ayudar a lograr "lo mejor de ambos mundos".

Una limitación más fundamental del enfoque general descrito en este documento (ampliar cualquier modelo tipo LM, ya sea agresivo o bidireccional) es que eventualmente puede toparse (o ya podría estar topando) con los límites de la 33.
objetivo de preentrenamiento. Nuestro objetivo actual pesa cada ficha por igual y carece de una noción de lo que es más importante y lo que es menos importante. [RRS20] demuestra los beneficios de personalizar la predicción para entidades de interés. Además, con objetivos auto supervisados, la especificación de la tarea se basa en forzar la tarea deseada a un problema de predicción, mientras que, en última instancia, los sistemas de lenguaje útiles (por ejemplo, asistentes virtuales) podrían considerarse mejor como tomar decisiones dirigidas a objetivos en lugar de simplemente hacer predicciones. Finalmente, los grandes modelos de lenguaje pre-entrenados no se basan en otros dominios de experiencia, como el video o la interacción física del mundo real, y por lo tanto carecen de una gran cantidad de contexto sobre el mundo [BHT + 20]. Por todas estas razones, es probable que escalar la predicción puramente auto supervisada alcance los límites, y es probable que sea necesario un aumento con un enfoque diferente. Las direcciones futuras prometedoras en este sentido podrían incluir aprender la función objetiva de los humanos [ZSW + 19a], afinar con aprendizaje de refuerzo o agregar modalidades adicionales como imágenes para proporcionar una base y un mejor modelo del mundo [CLY + 19]. Otra limitación Los modelos de lenguaje comparten ampliamente la pobre eficiencia de la muestra durante la capacitación previa. Si bien GPT-3 da un paso hacia la eficiencia de la muestra en el tiempo de prueba más cercana a la de los humanos (one-shot o zero-shot), todavía ve mucho más texto durante el pre-entrenamiento que un humano en su vida [Lin20]. Mejorar la eficiencia de la muestra previa al entrenamiento es una dirección importante para el trabajo futuro, y podría provenir de la conexión a tierra en el mundo físico para proporcionar información adicional, o de mejoras algorítmicas. Una limitación, o al menos la incertidumbre, asociada con el aprendizaje de pocos disparos en GPT-3 es ambigüedad acerca de si el aprendizaje de pocos disparos realmente aprende nuevas tareas "desde cero" en el momento de la inferencia, o si simplemente reconoce e identifica las tareas que ha aprendido durante el entrenamiento. Estas posibilidades existen en un espectro, que va desde demostraciones en el conjunto de entrenamiento que se extraen de la misma distribución que las del momento de la prueba, hasta reconocer la misma tarea pero en un formato diferente, hasta adaptarse a un estilo específico de una tarea general como QA, para aprender una habilidad completamente de novo. Donde GPT-3 está en este espectro también puede variar de una tarea a otra. Las tareas sintéticas, tales como descifrar palabras o definir palabras sin sentido, parecen especialmente aprendebles de novo, mientras que la traducción debe aprenderse claramente durante el entrenamiento previo, aunque posiblemente a partir de datos que son muy diferentes en organización y estilo que los datos de prueba. En última instancia, ni siquiera está claro qué aprenden los humanos desde cero frente a las demostraciones anteriores. Incluso organizar diversas demostraciones durante el entrenamiento previo e identificarlas en el momento de la prueba sería un avance para los modelos de lenguaje, pero sin embargo, comprender con precisión cómo funciona el aprendizaje con pocos disparos es una dirección importante inexplorada para futuras investigaciones. Una limitación asociada con los modelos a escala de GPT- 3, independientemente de la función objetivo o algoritmo, es que son costosos e inconvenientes para realizar inferencia, lo que puede presentar un desafío para la aplicabilidad práctica de los modelos de esta escala en su forma actual. Una posible dirección futura para abordar esto es la destilación [HVD15] de modelos grandes hasta un tamaño manejable para tareas específicas. Los modelos grandes como GPT-3 contienen una amplia gama de habilidades, la mayoría de las cuales no son necesarias para una tarea específica, lo que sugiere que, en principio, puede ser posible la destilación agresiva. La destilación está bien explorada en general [LHCG19a] pero no ha sido probado en la escala de cientos de miles de millones de parámetros; se pueden asociar nuevos desafíos y oportunidades con la aplicación a modelos de este tamaño. Finalmente, GPT-3 comparte algunas limitaciones comunes a la mayoría de los sistemas de aprendizaje profundo: sus decisiones no son fácilmente interpretables, es no necesariamente está bien calibrado en sus predicciones sobre entradas novedosas como lo observa la varianza mucho más alta en el rendimiento que los humanos en los puntos de referencia estándar, y conserva los sesgos de los datos sobre los que se ha entrenado. Este último problema, los sesgos en los datos que pueden llevar al modelo a generar contenido estereotipado o prejuicioso, es de especial preocupación desde una perspectiva social, y se discutirá junto con otros problemas en la próxima sección sobre Impactos más amplios (Sección 6).

6 Impactos más amplios Los modelos de idioma tienen una amplia gama de aplicaciones beneficiosas para la sociedad, incluyendo autocompletado de código y escritura, asistencia gramatical, generación de narrativa de juegos, mejora de las respuestas de los motores de búsqueda y respuesta a preguntas. Pero también tienen aplicaciones potencialmente dañinas. GPT-3 mejora la calidad de la generación de texto y la adaptabilidad de modelos más pequeños y aumenta la dificultad de distinguir el texto sintético del texto escrito por humanos. Por lo tanto, tiene el potencial de avanzar en las aplicaciones beneficiosas y perjudiciales de los modelos de lenguaje. Aquí nos enfocamos en los daños potenciales de los modelos de lenguaje mejorados, no porque creamos que los daños son necesariamente mayores, sino para estimular los esfuerzos para estudiarlos y mitigarlos. Los impactos más amplios de modelos de lenguaje como este son numerosos. Nos centramos en dos cuestiones principales: el potencial para el uso indebido deliberado de modelos de lenguaje como GPT-3 en la Sección 6.1, y cuestiones de sesgo, imparcialidad y representación dentro de modelos como GPT-3 en la Sección 6.2. También discutimos brevemente cuestiones de eficiencia energética (Sección 6.3)

6.1 Uso indebido de los modelos de lenguaje Los usos maliciosos de los modelos de lenguaje pueden ser algo difíciles de anticipar porque a menudo implican reutilizar modelos de lenguaje en un entorno muy diferente o con un propósito diferente al que pretendían los investigadores. Para ayudar con esto, podemos pensar en términos de los marcos tradicionales de evaluación de riesgos de seguridad, que describen pasos clave como identificar amenazas e impactos potenciales, evaluar la probabilidad y determinar el riesgo como una combinación de probabilidad e impacto [Ros12]. Discutimos tres factores: posibles aplicaciones de uso indebido, actores de amenazas y estructuras de incentivos externos. 6.1.1 Aplicaciones de uso indebido potencial Cualquier actividad socialmente dañina que se base en la generación de texto podría ser aumentada por poderosos modelos de lenguaje. Los ejemplos incluyen información errónea, spam, phishing, abuso de procesos legales y gubernamentales, redacción de ensayos académicos fraudulentos y pretextos de ingeniería social. Muchas de estas aplicaciones tienen un cuello de botella en los seres humanos para escribir textos de calidad suficientemente alta. Los modelos de lenguaje que producen una generación de texto de alta calidad podrían reducir las barreras existentes para llevar a cabo estas actividades y aumentar su eficacia. El potencial de mal uso de los modelos de lenguaje aumenta a medida que mejora la calidad de la síntesis de texto. La capacidad de GPT-3 para generar varios párrafos de contenido sintético que las personas encuentran difícil de distinguir del texto escrito en humanos en 3.9.4 representa un hito preocupante a este respecto. 6.1.2 Análisis de actores de amenazas Los actores de amenazas pueden organizarse por niveles de habilidad y recursos, que van desde actores poco o medianamente calificados y con recursos que pueden construir un producto malicioso hasta 'amenazas persistentes avanzadas' (APT): grupos altamente calificados y con buenos recursos (por ejemplo, patrocinados por el estado) con agendas a largo plazo [SBC + 19] Para comprender cómo piensan los actores de baja y media habilidad sobre los modelos de lenguaje, hemos estado monitoreando foros y grupos de chat donde se discuten con frecuencia las tácticas de desinformación, la distribución de malware y el fraude informático. Si bien encontramos una discusión significativa sobre el mal uso después del lanzamiento inicial de GPT-2 en la primavera de 2019, encontramos menos casos de experimentación y no hubo implementaciones exitosas desde entonces. Además, esas discusiones de mal uso se correlacionaron con la cobertura mediática de las tecnologías de modelos de lenguaje. A partir de esto, evaluamos que la amenaza de mal uso por parte de estos actores no es inmediata, pero las mejoras significativas en la confiabilidad podrían cambiar esto. Debido a que los APT generalmente no discuten las operaciones al aire libre, hemos consultado con analistas de amenazas profesionales sobre la posible actividad de APT que implica el uso de Modelos de lenguaje. Desde el lanzamiento de GPT-2 no ha habido una diferencia discernible en las operaciones que pueden ver ganancias potenciales al usar modelos de lenguaje. La evaluación fue que los modelos de lenguaje pueden no valer la pena invertir recursos significativos porque no ha habido una demostración convincente de que los modelos de lenguaje actuales son significativamente mejores que los métodos actuales para generar texto, y porque los métodos para "enfocar" o "controlar" el contenido de los modelos de lenguaje son todavía en una etapa muy temprana 6.1.1. Estructuras de incentivos externos Cada grupo de actores de amenazas también tiene un conjunto de tácticas, técnicas y procedimientos (TTP) en los que confían para realizar sus modificaciones. Los TTP están influenciados por factores económicos como la escalabilidad y la facilidad de implementación; El phishing es extremadamente popular en todos los grupos porque ofrece un método de bajo costo, bajo esfuerzo y alto rendimiento para implementar malware y robar credenciales log. El uso de modelos de lenguaje para aumentar los TTP existentes probablemente resultaría en un costo de implementación aún más bajo. La facilidad de uso es otro incentivo significativo. Tener una infraestructura estable tiene un gran impacto en la adopción de TTP. Sin embargo, los resultados de los modelos de lenguaje son estocásticos, y aunque los desarrolladores pueden restringirlos (por ejemplo, utilizando la función de truncado superior), no pueden funcionar de manera consistente sin comentarios humanos. Si un bot de desinformación de redes sociales produce resultados que son confiables el 99% del tiempo, pero produce resultados incoherentes el 1% del tiempo, esto podría reducir la cantidad de trabajo humano requerido para operar este bot. Pero todavía se necesita un humano para filtrar los resultados, lo que restringe la escalabilidad de la operación. Según nuestro análisis de este modelo y el análisis de los actores de la amenaza y el paisaje, sospechamos que los investigadores de IA eventualmente desarrollarán modelos de lenguaje que sean lo suficientemente consistentes y orientables que Serán de mayor interés para los actores maliciosos. Esperamos que esto presente desafíos para la comunidad de investigación más amplia, y esperamos trabajar en esto a través de una combinación de investigación de mitigación, creación de prototipos y coordinación con otros desarrolladores técnicos.

6.2 Equidad, sesgo y representación Los sesgos presentes en los datos de capacitación pueden llevar a los modelos a generar contenido estereotipado o prejuicioso. Esto es preocupante, ya que el sesgo del modelo podría dañar a las personas en los grupos relevantes de diferentes maneras al afianzar los estereotipos existentes y producir representaciones degradantes entre otros daños potenciales [Cra17]. Hemos realizado un análisis de sesgos en el modelo para comprender mejor las limitaciones de GPT-3 cuando se trata de equidad, sesgo y representación.8 Nuestro objetivo no es caracterizar exhaustivamente GPT-3, sino dar un análisis preliminar de algunos de sus limitaciones y comportamientos. Nos centramos en los prejuicios relacionados con el género, la raza y la religión, aunque es probable que existan muchas otras categorías de prejuicios y que puedan estudiarse en el trabajo de seguimiento. Este es un análisis preliminar y no refleja todas las sesgos del modelo, incluso dentro de las categorías estudiadas. En términos generales, nuestro análisis indica que los modelos capacitados en Internet tienen sesgos a escala de Internet; Los modelos tienden a reflejar los estereotipos presentes en sus datos de entrenamiento. A continuación discutimos nuestros hallazgos preliminares de sesgo a lo largo de las dimensiones de género, raza y religión. Investigamos el sesgo en el modelo de parámetros de 175 mil millones y también en modelos similares más pequeños, para ver si son diferentes en esta dimensión y cómo. 6.2.1 Género En nuestra investigación del sesgo de género en GPT-3, nos centramos en las asociaciones entre género y ocupación. . Descubrimos que las ocupaciones en general tienen una mayor probabilidad de ser seguidas por un identificador de género masculino que uno femenino (en otras palabras, son hombres inclinados) cuando se les da un contexto como "La {ocupación} era una" (Variante neutral). El 83% de las 388 ocupaciones que probamos tenían más probabilidades de ser seguidas por un identificador masculino por GPT-3. Medimos esto alimentando al modelo con un contexto como "El detective era un" y luego observando la probabilidad de que el modelo siguiera con palabras indicadoras masculinas (por ejemplo, hombre, hombre, etc.) o palabras indicadoras femeninas (mujer, mujer, etc.) En particular, las ocupaciones que demostraban niveles más altos de educación, como legislador, banquero o profesor emérito, eran muy inclinadas por los hombres junto con ocupaciones que requieren trabajo físico duro, como albañil, ingeniero de fábrica y sheriff. Las ocupaciones que tenían más probabilidades de ser seguidas por identificadores femeninos incluyen la partera, la enfermera, la recepcionista, la ama de llaves, etc. También probamos cómo cambiaron estas probabilidades cuando cambiamos el contexto a "La {ocupación} competente era una" (Variante Competente), y cuando cambiamos el contexto para ser "La incompetente {ocupación} era una" (Variante incompetente) para cada ocupación en el conjunto de datos. Descubrimos que, cuando se le solicitó "La {ocupación} competente era una", la mayoría de las ocupaciones tenían una probabilidad aún mayor de ser seguidas por un identificador de amale que una femenina que el caso con nuestro mensaje neutral original, "La {ocupación }era un". Con el aviso "La incompetente {ocupación} era" la mayoría de las ocupaciones todavía se inclinaban hacia los hombres con una probabilidad similar a la de nuestro aviso neutral original. El sesgo de ocupación promedio, medido como1njobs∑jobslog (P (femenino | Contexto) P (masculino | Contexto))) - fue − 1.11 para la variante neutral, −2.14 para la variante competente y − 1.15 para la variante incompetente. También llevamos resolvió el pronombre en el conjunto de datos Winogender [RNLVD18] utilizando dos métodos que corroboraron la tendencia del modelo de asociar la mayoría de las ocupaciones con los hombres. Un método midió la capacidad de los modelos para asignar correctamente un pronombre como la ocupación o el participante. Por ejemplo, alimentamos el contexto de modela como "El asesor se reunió con el asesor porque quería obtener consejos sobre aplicaciones de trabajo. 'Ella' se refiere a" y encontró la opción con la probabilidad más baja entre las dos opciones posibles (Opciones entre la Opción de ocupación: asesor; Opción de participante: asesor). Las palabras de ocupación y participante a menudo tienen sesgos sociales asociados con ellos, como la suposición de que los mostocupantes son por defecto hombres. Descubrimos que los modelos de lenguaje aprendieron algunos de estos sesgos, como la tendencia a asociar los pronombres femeninos con las posiciones de los participantes más que los pronombres masculinos. GPT-3 175B tuvo la mayor precisión de todos los modelos (64.17%) en esta tarea. También fue el único modelo donde la precisión para las oraciones de Ocupante (oraciones donde la respuesta correcta era la opción de Ocupación) para las mujeres fue mayor que para los hombres (81.7% vs 76.7%). Todos los demás modelos tenían una mayor precisión para los pronombres masculinos con oraciones de ocupación en comparación con los pronombres femeninos, con la excepción de nuestro segundo modelo más grande, GPT-3 13B, que tenía la misma precisión (60%) para ambos. Esto ofrece algunas pruebas preliminares de que en los lugares donde los problemas de sesgo pueden hacer que los modelos de lenguaje sean susceptibles de error, los modelos más grandes son más robustos que los modelos más pequeños. También realizamos pruebas de coincidencia, donde analizamos qué palabras es probable que ocurran cerca de otros palabras preseleccionadas Creamos un modelo de salida samEl conjunto se genera generando 800 salidas de longitud 50 cada una con una temperatura 8 Evaluar la equidad, el sesgo y la representación en modelos de lenguaje es un área de rápido desarrollo con un gran cuerpo de trabajo previo. Ver, por ejemplo, [HZJ + 19, NBR20, SCNP19] .

TABLA

de 1 y topp de 0.9 para cada solicitud en nuestro conjunto de datos. En cuanto al género, tuvimos mensajes como "Él era muy", "Ella era muy", "Sería descrito como", "Ella sería descrita como" 9. Observamos los adjetivos y los verbos en las 100 palabras más favoritas usando un etiquetador POS estándar [LB02]. Descubrimos que las mujeres a menudo se describían usando palabras orientadas a la apariencia como "hermosa" y "hermosa" en comparación con los hombres que a menudo se describían con adjetivos que abarcan un espectro mayor. La Tabla 6.1 muestra las 10 palabras descriptivas más favorecidas para el modelo junto con El número bruto de veces que cada palabra co-ocurrió con un indicador de pronombre. "Más favorecido" aquí indica las palabras que estaban más sesgadas hacia una categoría al coexistir con ella a un ritmo más alto en comparación con la otra categoría. Para poner estos números en perspectiva, también hemos incluido el promedio del número de coincidencias en todas las palabras que califican para cada género.

6.2.2 Raza Para investigar el sesgo racial en GPT-3, sembró el modelo con indicaciones tales como: "El hombre {raza} era muy", "La mujer {raza} era muy" y "La gente describiría a la persona {raza} como "y generó 800 muestras para cada una de las indicaciones anteriores, con {raza} reemplazado por un término que indica una categoría racial como Whiteor Asian. Luego medimos las coincidencias de palabras en las muestras generadas. Dada la investigación previa que demuestra que los modelos de lenguaje producen texto de diferentes sentimientos cuando varían características como la ocupación [HZJ + 19], exploramos cómo el sentimiento afectado por la raza. Medimos el sentimiento usando Senti WordNet [BES10] para las palabras que ocurrieron de manera desproporcionada con cada raza. El sentimiento de cada palabra varió de 100 a -100, con puntajes positivos que indican palabras positivas (por ejemplo, maravilla: 100, amigable: 87.5), puntajes negativos que indican palabras negativas (por ejemplo, miserable: -87.5, horrible: -87.5) y una puntuación de 0 indicando palabras neutrales (p. ej., pendiente, chalet). Debe tenerse en cuenta que explícitamente pedimos a los modelos que hablen sobre la raza y esto a su vez generó un texto centrado en las características raciales; Estos resultados no provienen de los modelos que hablan de la raza en estado salvaje, sino de la raza en una configuración experimental en la que han sido preparados para hacerlo. Además, dado que estamos midiendo el sentimiento simplemente observando las coincidencias de palabras, el sentimiento resultante puede reflejar factores sociohistóricos; por ejemplo, el texto relacionado con una discusión sobre la esclavitud con frecuencia tendrá un sentimiento negativo, lo que puede conducir a una asociación demográfica asociada con un sentimiento negativo bajo esta metodología de prueba. En los modelos que analizamos, 'Asian' tuvo un sentimiento consistentemente alto: ocupó el primer lugar en 3 de 7 modelos. Por otro lado, 'Black' tenía un sentimiento consistentemente bajo: se clasificó como el más bajo en 5 de 7 modelos. Estas diferencias se redujeron marginalmente en los tamaños de modelo más grandes. Este análisis da una idea de los sesgos de los diferentes modelos y destaca la necesidad de un análisis más sofisticado de la relación entre el sentimiento, las entidades y los datos de entrada.9 Solo utilizamos pronombres masculinos y femeninos. Esta suposición simplificadora hace que sea más fácil estudiar la coincidencia, ya que no requiere el aislamiento de instancias en las que 'ellos' se refieren a un sustantivo singular de aquellos donde no lo hizo, pero es probable que otras formas de sesgo de género estén presentes y puedan estudiarse utilizando diferentes aproximaciones.

FIGURA

TABLA

6.2.3 Religión Estudiamos qué palabras coexistían con términos religiosos relacionados con el ateísmo, el budismo, el cristianismo, el hinduismo, el islam y el judaísmo, generando 800 salidas modelo de longitud ≈50 con una temperatura de 1 y un toppof de 0.9 para cada solicitud. Nuestros avisos eran de la naturaleza "{Los practicantes de la religión} son" (Ej. "Los cristianos son") para cada una de las seis categorías religiosas mencionadas anteriormente. Luego permitimos que el modelo llevara a cabo terminaciones de forma natural y creamos un conjunto de tales terminaciones para estudiar la coincidencia de palabras. El siguiente es un ejemplo de salida del modelo: "Los budistas se dividen en dos ramas principales: Theravada y Mahayana. Theravada es la más conservadora , centrándose en la vida monástica y los primeros sutras y negándose a reconocer los sutras posteriores de Mahayana como auténticos. "Similar a la raza, encontramos que los modelos hacen asociaciones con términos religiosos que indican cierta propensión a reflejar cómo estos términos a veces se presentan en el mundo. Por ejemplo, con el religionIslam, encontramos que las palabras suchasramadan, profetandmosqueco-ocurrieron a un ritmo mayor que para otras religiones. También encontramos que las palabras como violento, terrorista y terrorista ocurrieron a un ritmo mayor con el Islam que con otras religiones y estaban entre las 40 palabras más favorecidas para el Islam en GPT-3

6.2.4 Sesgos futuros y desafíos de equidad Hemos presentado este análisis preliminar para compartir algunos de los sesgos que encontramos con el fin de motivar más investigaciones y para resaltar las dificultades inherentes a la caracterización de sesgos en modelos generativos a gran escala; esperamos que esto sea un área de investigación continua para nosotros y estamos entusiasmados de discutir diferentes enfoques metodológicos con la comunidad. Vemos el trabajo en esta sección como señalización subjetiva: elegimos género, raza y religión como punto de partida, pero reconocemos el subjetividad inherente en esta elección. Nuestro trabajo está inspirado en la literatura sobre la caracterización de atributos de modelos para desarrollar etiquetas informativas tales como Tarjetas Modelo para Informes Modelo de [MWZ + 18]. En última instancia, es importante no solo caracterizar sesgos en los sistemas de lenguaje sino también intervenir. La literatura sobre esto también es extensa [QMZH19, HZJ + 19], por lo que ofrecemos solo unos breves comentarios sobre direcciones futuras específicas para modelos de lenguaje grande. Con el fin de allanar el camino para la prevención efectiva del sesgo en los modelos de propósito general, existe la necesidad de construir un vocabulario común que vincule los desafíos normativos, técnicos y empíricos de la mitigación del sesgo para estos modelos. Hay espacio para más investigación que se relacione con la literatura fuera de la PNL, articule mejor las declaraciones normativas sobre el daño y se relacione con la experiencia vivida de las comunidades afectadas por los sistemas de PNL [BBDIW20]. Por lo tanto, el trabajo de mitigación no debe abordarse exclusivamente con un objetivo impulsado por la métrica para 'eliminar' el sesgo, ya que se ha demostrado que tiene puntos ciegos [GG19, NvNvdG19] pero de manera holística. 6.3 Uso de energía El pre-entrenamiento práctico a gran escala requiere grandes cantidades de cómputo, que requiere mucha energía: entrenamiento del GPT-3175B Consumió varios miles de petaflop / s-días de cómputo durante el pre-entrenamiento, en comparación con decenas de petaflop / s-días para un modelo GPT-2 de 1.5B parámetro (Figura 2.2). Esto significa que deberíamos ser conscientes del costo y la eficiencia de tales modelos, como lo defiende [SDSE19]. El uso de la capacitación previa a gran escala también ofrece otra lente a través de la cual ver la eficiencia de los modelos grandes; deberíamos considerar no solo los recursos que van a entrenarlos, pero cómo estos recursos se amortizan durante la vida útil de un modelo, que posteriormente se utilizará para una variedad de propósitos y se ajustará para tareas específicas. Aunque los modelos como GPT-3 consumen recursos significativos durante el entrenamiento, pueden ser sorprendentemente eficientes una vez entrenados: incluso con el GPT-3 175B completo, generar 100 páginas de contenido de un modelo entrenado puede costar del orden de 0.4 kW-hr, o solo unos pocos centavos en costos de energía. Además, técnicas como la destilación de modelos [LHCG19a] pueden reducir aún más el costo de dichos modelos, permitiéndonos adoptar un paradigma de capacitación de modelos únicos a gran escala, y luego crear versiones más eficientes de ellos para su uso en contextos apropiados. El progreso algorítmico también puede, naturalmente, aumentar aún más la eficiencia de dichos modelos a lo largo del tiempo, similar a las tendencias observadas en el reconocimiento de imágenes y la traducción automática neural [HB20].

7 Trabajo relacionado Varias líneas de trabajo se han centrado en aumentar el recuento de parámetros y / o el cálculo en modelos de lenguaje como medios para mejorar el rendimiento generativo o de tareas. Un trabajo inicial amplió los modelos de lenguaje basados ​​en LSTM a más de mil millones de parámetros [JVS + 16]. Una línea de trabajo aumenta directamente el tamaño de los modelos de transformadores, ampliando los parámetros y los FLOPS por token aproximadamente en proporción. El trabajo en este sentido ha aumentado sucesivamente el tamaño del modelo: 213 millones de parámetros [VSP + 17] en el documento original, 300 millones de parámetros [DCLT18], 1.5 billones de parámetros [RWC + 19], 8 billones de parámetros [SPP + 19], 11 billones parámetros [RSR + 19], y más recientemente 17 mil millones de parámetros [Tur20]. Una segunda línea de trabajo se ha centrado en aumentar el recuento de parámetros, pero no el cálculo, como un medio de aumentar la capacidad de los modelos para almacenar información sin un mayor costo computacional. Estos enfoques se basan en el marco de cálculo condicional [BLC13] y específicamente, el método de mezcla de expertos [SMM + 17] se ha utilizado para producir 100 mil millones de modelos de parámetros y más recientemente 50 mil millones de modelos de traducción de parámetros [AJF19], aunque solo una pequeña fracción de los parámetros se usan realmente en cada pase hacia adelante. Un tercer enfoque aumenta la computación sin aumentar los parámetros; ejemplos de este enfoque incluyen el tiempo de cálculo adaptativo [Gra16] y el transformador universal [DGV + 18]. Nuestro trabajo se enfoca en el primer enfoque (escalando el cómputo y los parámetros juntos, haciendo que la red neuronal sea más grande) y aumenta el tamaño del modelo 10 veces más que los modelos anteriores que emplean esta estrategia. Varios esfuerzos también han estudiado sistemáticamente el efecto de la escala en el rendimiento del modelo de lenguaje. [KMH + 20, RRBS19, LWS + 20, HNA + 17], encuentran una tendencia suave de la ley de poder en la pérdida a medida que se amplían los modelos de lenguaje autorregresivo. Este trabajo sugiere que esta tendencia continúa en gran medida a medida que los modelos continúan aumentando (aunque un tal vez se pueda detectar una ligera curvatura de la curva en la Figura 3.1), y también encontramos incrementos relativamente suaves en muchas (aunque no todas) tareas posteriores en 3 órdenes de magnitud de escala. Otra línea de trabajo va en la dirección opuesta a la escala, intentando para preservar un rendimiento sólido en modelos de lenguaje que sean lo más pequeños posible. Este enfoque incluye ALBERT [LCG + 19], así como general [HVD15] y39
enfoques específicos de la tarea [SDCW19, JYS + 19, KR16] para la destilación de modelos de lenguaje. Estas arquitecturas y técnicas son potencialmente complementarias a nuestro trabajo y podrían aplicarse para disminuir la latencia y la huella de memoria de los modelos gigantes.

Como los modelos lingüísticos afinados se han acercado al rendimiento humano en muchas tareas estándar de referencia, se ha dedicado un esfuerzo considerable a construir tareas más difíciles o abiertas, incluida la respuesta a preguntas [KPR + 19, IBGC + 14, CCE + 18, MCKS18], lectura comprensión [CHI + 18, RCM19], y conjuntos de datos construidos de forma contradictoria diseñados para ser difíciles para los modelos de lenguaje existentes [SBBC19, NWD + 19]. En este trabajo probamos nuestros modelos en muchos de estos conjuntos de datos. Muchos esfuerzos anteriores se han centrado específicamente en responder preguntas, lo que constituye una fracción significativa de las tareas que probamos. Los esfuerzos recientes incluyen [RSR + 19, RRS20], que ajustó un modelo de lenguaje de parámetros de 11 mil millones, y [GLT + 20], que se centró en asistir a un gran corpus de datos en el momento de la prueba. Nuestro trabajo difiere en enfocarse en el aprendizaje en contexto, pero podría combinarse en el futuro con los de [GLT + 20, LPP + 20]. El aprendizaje en modelos de lenguaje se ha utilizado en [RWC + 19], aunque con resultados mucho más limitados y estudio nosystematic En términos más generales, el metaaprendizaje del modelo de lenguaje tiene una estructura de bucle interno-bucle externo, lo que lo hace estructuralmente similar al metaaprendizaje aplicado a ML en general. Aquí hay una extensa literatura, que incluye redes coincidentes [VBL + 16], RL2 [DSC + 16], aprendiendo a optimizar [RL16, ADG + 16, LM17] y MAML [FAL17]. Nuestro enfoque de rellenar el contexto del modelo con ejemplos anteriores es más estructuralmente similar a RL2 y también se asemeja a [HYC01], en el sentido de que se lleva a cabo un bucle interno de adaptación a través del cálculo en las activaciones del modelo a través de pasos de tiempo, sin actualizar los pesos, mientras que un bucle externo (en este caso solo pre-entrenamiento del modelo de lenguaje) se actualiza los pesos e implícitamente aprende la capacidad de adaptarse o al menos reconocer las tareas definidas en el tiempo de inferencia. Se exploró la estimación de densidad autorregresiva de pocos disparos en [RCP + 17] y [GWC + 18] estudiaron NMT asa de bajo recurso problema de aprendizaje de pocos disparos. Si bien el mecanismo de nuestro enfoque de pocos disparos es diferente, el trabajo previo también ha explorado formas de usar modelos de lenguaje pre-entrenados en combinación con el descenso de gradiente para realizar el aprendizaje de pocos disparos [SS20]. Otro subcampo con objetivos similares es el aprendizaje semi-supervisado, donde enfoques como UDA [XDH + 19] también exploran métodos de ajuste cuando hay muy pocos datos etiquetados disponibles. Dar instrucciones de modelos de tareas múltiples en lenguaje natural se formalizó primero en un sistema supervisado ajuste con [MKXS18] y utilizado para algunas tareas (como resumir) en un modelo de lenguaje con [RWC + 19]. La noción de presentar tareas en lenguaje natural también se exploró en el transformador de texto a texto [RSR + 19], aunque allí se aplicó el ajuste fino de tareas múltiples en lugar del aprendizaje en contexto sin actualizaciones de peso. Otro enfoque para aumentar la generalidad y la capacidad de transferencia de aprendizaje en modelos de lenguaje es el aprendizaje de tareas múltiples [Car97], que ajusta una mezcla de tareas posteriores juntas, en lugar de actualizar los pesos por separado. Si el aprendizaje exitoso de tareas múltiples podría permitir el uso de un solo modelo para muchas tareas sin actualizar los pesos (similar a nuestro enfoque de aprendizaje en contexto), o alternativamente podría mejorar la eficiencia de la muestra al actualizar los pesos para una nueva tarea. El aprendizaje de tareas múltiples ha mostrado algunos resultados iniciales prometedores [LGH + 15, LSP + 18] y el ajuste fino de etapas múltiples se ha convertido recientemente en una parte estandarizada de los resultados de SOTA en algunos conjuntos de datos [PFB18] y ha superado los límites en ciertas tareas [KKS + 20 ], pero aún está limitado por la necesidad de seleccionar manualmente colecciones de conjuntos de datos y configurar currículos de capacitación. Por el contrario, la capacitación previa a una escala lo suficientemente grande parece ofrecer una amplia distribución “natural” de tareas implícitamente contenidas en la predicción del texto mismo. Una dirección para el trabajo futuro podría ser intentar generar un conjunto más amplio de tareas explícitas para el aprendizaje de tareas múltiples, por ejemplo a través de la generación de procedimientos [TFR + 17], la interacción humana [ZSW + 19b] o el aprendizaje activo [Mac92]. Innovación algorítmica en el lenguaje los modelos en los últimos dos años han sido enormes, incluida la direccionalidad basada en la eliminación de ruido [DCLT18], el prefijoLM [DL15] y las arquitecturas codificador-decodificador [LLG + 19, RSR + 19], permuciones aleatorias durante el entrenamiento [YDY + 19], arquitecturas que mejoran la eficiencia del muestreo [DYY + 19], mejoras en los datos y los procedimientos de capacitación [LOG + 19], y aumenta la eficiencia en los parámetros de inclusión [LCG + 19]. Muchas de estas técnicas proporcionan ganancias significativas en tareas posteriores. En este trabajo continuamos enfocándonos en modelos de lenguaje autorregresivo puro, tanto para enfocarnos en el rendimiento de aprendizaje en contexto como para reducir la complejidad de nuestras implementaciones de modelos grandes. Sin embargo, es muy probable que la incorporación de estos avances algorítmicos pueda mejorar el rendimiento de GPT-3 en tareas posteriores, especialmente en la configuración de ajuste fino, y combinar la escala de GPT-3 con estas técnicas algorítmicas es una dirección prometedora para el trabajo futuro.

8 Conclusión: Presentamos un modelo de lenguaje de parámetros de 175 mil millones que muestra un alto rendimiento en muchas tareas de PNL y puntos de referencia en la configuración de disparo cero, disparo único y pocos disparos, en algunos casos casi igualando el rendimiento de 40
sistemas de vanguardia afinados, además de generar muestras de alta calidad y sólidas tareas de rendimiento cualitativo definidas sobre la marcha. Documentamos tendencias más o menos predecibles de escala en el rendimiento sin utilizar ajustes precisos. También discutimos los impactos sociales de esta clase de modelo. A pesar de muchas limitaciones y debilidades, estos resultados sugieren que los modelos de lenguaje muy grandes pueden ser un ingrediente importante en el desarrollo de sistemas adaptables de lenguaje general. Agradecimientos Los autores desean agradecer a Ryan Lowe por brindar comentarios detallados sobre los borradores del documento. Gracias a JakubPachocki y Szymon Sidor por sugerir tareas, y Greg Brockman, Michael Petrov, Brooke Chan y ChelseaVoss por ayudar a realizar evaluaciones en la infraestructura de OpenAI. Gracias a David Luan por el apoyo inicial en la ampliación de este proyecto, Irene Solaiman por las discusiones sobre formas de abordar y evaluar el sesgo, Harrison Edwards y YuraBurda por las discusiones y experimentación con el aprendizaje en contexto, Geoffrey Irving y Paul Christiano por las primeras discusiones sobre la escala del modelo de lenguaje, Long Ouyang por asesorar sobre el diseño de los experimentos de evaluación humana, Chris Hallacy por las discusiones sobre la recopilación de datos y Shan Carter por su ayuda con el diseño visual. Gracias a los millones de personas que crearon contenido que se utilizó en la capacitación del modelo, y a aquellos que participaron en la indexación o la votación del contenido (en el caso de WebText). Además, nos gustaría agradecer a toda la infraestructura de OpenAI y a los equipos de supercomputación por hacer posible entrenar modelos a esta escala.

Contribuciones Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler y Jeffrey Wu implementaron los modelos a gran escala, la infraestructura de entrenamiento y las estrategias paralelas a los modelos. Tom Brown, Dario Amodei, Ben Mann y Nick Ryder realizaron pre -Ben Mann y Alec Radford recolectaron, filtraron, deduplicaron y realizaron análisis de superposición en los datos de entrenamiento. Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan y Girish Sastry implementaron las tareas posteriores y El marco de software para apoyarlos, incluida la creación de tareas sintéticas. Jared Kaplan y Sam McCandlish inicialmente predijeron que un modelo de lenguaje gigante debería mostrar ganancias continuas, y leyes de escalado aplicadas para ayudar a predecir y guiar las decisiones de escalado de datos y modelos para la investigación. durante el entrenamiento. Alec Radfordor demostró originalmente que el aprendizaje de pocos disparos ocurre en modelos de lenguaje. d Kaplan y Sam McCandlish demostraron que los modelos más grandes aprenden más rápidamente en contexto, y estudiaron sistemáticamente las curvas de aprendizaje en contexto, la solicitud de tareas y los métodos de evaluación. Prafulla Dhariwal implementó una versión temprana de la base de código y desarrolló las optimizaciones de memoria para un entrenamiento de precisión total. Rewon Child y Mark Chende desarrollaron una versión temprana de nuestra estrategia de modelo paralelo. Rewon Child y Scott Gray contribuyeron con el transformador disperso. Aditya Rameshe experimentó con estrategias de escalado de pérdidas para el entrenamiento previo. Melanie Subbiah y Arvind Neelakanta implementaron, experimentaron y probaron la búsqueda de haz. Pranav Shyam trabajó en SuperGLUE y asistió con conexiones a literatura de aprendizaje y metaaprendizaje de pocos disparos. Sandhini Agarwal realizó el análisis de equidad y representación. Girish Sastry y Amanda Askell realizaron las evaluaciones humanas del modelo. Ariel Herbert-Voss realizó el análisis de amenazas de uso malicioso. Gretchen Kruegeredited y red-teamed las secciones de política de Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin y Christopher Berneroptimizaron los clústeres de OpenAI para ejecutar los modelos más grandes de manera eficiente. Scott Gray desarrolló núcleos de GPU rápidos utilizados durante el entrenamiento. Jack Clarkled el análisis de los impactos éticos - equidad y representación , evaluaciones humanas del modelo y análisis de impactos más amplios, y asesoró a Gretchen, Amanda, Girish, Sandhini y Ariel en su trabajo.Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell , Girish Sastry y Jack Clark escribieron el artículo. Sam McCandlishled el análisis del escalado del modelo, y aconsejó a Tom Henighan y Jared Kaplan sobre su trabajo. Alec Radfordad supervisó el proyecto desde una perspectiva de PNL, sugirió tareas, puso los resultados en contexto y demostró el beneficio de la pérdida de peso para el entrenamiento.Ilya Sutskever fue una de las primeras defensoras de escalar grandes modelos de probabilidad generativa, y aconsejó a Pranav, Prafulla, Rewon, Alec y A ditya en su trabajo. Darío Amo diseñó y dirigió la investigación.

A Detalles del filtrado de rastreo común
B Detalles del entrenamiento del modelo
C Detalles de los estudios de contaminación del conjunto de prueba
D Computación total utilizada para entrenar modelos de lenguaje
E Evaluación de la calidad humana de artículos de noticias sintéticos
F Muestras adicionales de GPT-3
G Detalles de redacción de tareas y especificaciones
H Resultados en todas las tareas para todos los tamaños de modelo


