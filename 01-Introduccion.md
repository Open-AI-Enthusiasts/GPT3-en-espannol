## 1 Introducción 

Los sistemas de PNL han visto en los ultimos años una tendencia hacia el uso de modelos de lenguaje pre-entrenados, aplicadas en formas cada vez más flexibles e independientes de la tarea o tambien conocidos como tarea-agnósticos para _downstream transfer_. Primero, las representaciones de una sola capa se aprendieron usando vectores para cada palabra (_wordvectors_) [MCCD13](https://arxiv.org/pdf/2005.14165.pdf#cite.mikolov2013efficient), [PSM14](https://arxiv.org/pdf/2005.14165.pdf#cite.pennington2014glove) y alimentaron arquitecturas de tareas específicas, luego se usaron RNN con múltiples capas de representaciones y estado contextual, para formar representaciones más fuertes [DL15](https://arxiv.org/pdf/2005.14165.pdf#cite.dai2015semi), [MBXS17](https://arxiv.org/pdf/2005.14165.pdf#cite.mccann2017learned), [PNZtY18](https://arxiv.org/pdf/2005.14165.pdf#cite.peters2018dissecting) aunque todavía se aplica a arquitecturas de tareas específicas), y más recientemente los modelos de lenguaje recurrente o transformador pre-entrenados [VSP + 17](https://arxiv.org/pdf/2005.14165.pdf#cite.vaswani2017attention) se han ajustado directamente (_fine tuned_), eliminando por completo la necesidad de arquitecturas de tareas específicas [RNSS18](https://arxiv.org/pdf/2005.14165.pdf#cite.radford2018gpt1), [DCLT18](https://arxiv.org/pdf/2005.14165.pdf#cite.devlin2018bert), [HR18](https://arxiv.org/pdf/2005.14165.pdf#cite.howard2018universal). 

Este último paradigma ha llevado a un progreso sustancial en muchas tareas desafiantes para PNL, como comprensión de lectura, resolver preguntas, implicación textual y muchas otras, y ha continuado avanzando con base en nuevas arquitecturas y algoritmos [RSR + 19](https://arxiv.org/pdf/2005.14165.pdf#cite.raffel2019t5), [LOG + 19](https://arxiv.org/pdf/2005.14165.pdf#cite.liu2019roberta), [YDY + 19](https://arxiv.org/pdf/2005.14165.pdf#cite.yang2019xlnet), [LCG + 19](https://arxiv.org/pdf/2005.14165.pdf#cite.lan2019albert). Sin embargo, una limitación importante de este enfoque es que, si bien la arquitectura es independiente de la tarea, todavía existe la necesidad de _datasets_ y _fine tuning_ de tareas específicas: para lograr un rendimiento sólido en una tarea deseada generalmente se requiere un _fine tuning_ en un _dataset_ de miles a cientos de miles de ejemplos específicos para esa tarea. Eliminar esta limitación sería deseable, por varias razones. 

Primero, desde una perspectiva práctica, la necesidad de un gran conjunto de datos de ejemplos etiquetados para cada nueva tarea limita la aplicabilidad de los modelos de lenguaje. Existe una amplia gama de tareas de lenguaje útiles, que abarcan desde corregir la gramática, generar ejemplos de un concepto abstracto, hasta criticar una historia corta. Para muchas de estas tareas es difícil recopilar un gran _dataset_ de de entrenamiento supervisado, especialmente cuando el proceso debe repetirse para cada tarea nueva. 

En segundo lugar, el potencial para explotar correlaciones espurias en los datos de entrenamiento crece fundamentalmente con la expresividad del modelo y la estrechez de la distribución de entrenamiento(revisar). Esto puede crear problemas para el paradigma de ajuste previo sumado a _fine tuning_, donde los modelos están diseñados para ser grandes para absorber información durante el pre-entrenamiento, pero luego se ajustan _fine tuning_ en distribuciones de tareas muy estrechas. Por ejemplo, [HLW + 20](https://arxiv.org/pdf/2005.14165.pdf#cite.hendrycks2020pretrained) observe que los modelos más grandes no necesariamente generalizan mejor fuera de la distribución. Hay evidencia que sugiere que la generalización lograda bajo este paradigma puede ser pobre porque el modelo es demasiado específico para la distribución del entrenamiento y no se generaliza bien fuera de él [YdC + 19](https://arxiv.org/pdf/2005.14165.pdf#cite.yogatama2019learning), [MPL19](https://arxiv.org/pdf/2005.14165.pdf#cite.mccoy2019right). Por lo tanto, el rendimiento de los modelos _fine tuned_, en _benchmarks_ especificos, incluso cuando esta nominalmente a nivel humano, puede exagerar el rendimiento real en la tarea subyacente [GSL + 18](https://arxiv.org/pdf/2005.14165.pdf#cite.mccoy2019right), [NK19](https://arxiv.org/pdf/2005.14165.pdf#cite.niven2019probing). 

En tercer lugar, los humanos no requieren grandes conjuntos de datos supervisados para aprender la mayoria de tareas del lenguaje: - una breve directiva en lenguaje natural (p. ej., "por favor dígame si esta oración describe algo feliz o algo triste") o como máximo un pequeño número de demostraciones (p. ej. "aquí hay dos ejemplos de personas que actúan valientemente; por favor, dé un tercer ejemplo valentía ") es a menudo suficiente para permitir que un humano realice una nueva tarea al menos con un grado razonable de competencia. Además de señalar una limitación conceptual en nuestras técnicas actuales de PNL, esta adaptabilidad tiene ventajas prácticas: permite a los humanos mezclar sin problemas o cambiar entre muchas tareas y habilidades, por ejemplo, realizar sumas durante un largo diálogo. Para ser ampliamente útiles, algún día nos gustaría que nuestros sistemas de PNL tengan esta misma fluidez y generalidad. 

![gpt3-01](https://user-images.githubusercontent.com/12854504/88138119-ade05980-cbb2-11ea-90b3-1d8c29ce19d7.png)

**Figura 1.1: Meta-aprendizaje del modelo de lenguaje.** Durante el pre-entrenamiento no supervisado, un modelo de lenguaje desarrolla un amplio conjunto de habilidades y reconocimiento de patrones. Luego utiliza estas habilidades en momento de inferencia para adaptarse rápidamente o reconocer la tarea deseada. Usamos el término "aprendizaje en contexto" para describir el ciclo interno de este proceso, que ocurre a medida que avanza cada secuencia. Las secuencias en este diagrama no pretenden ser representativas de los datos que vería un modelo durante el pre-entrenamiento, pero tienen la intención de mostrar que a veces hay sub-tareas repetidas embebidas en una  secuencia sencilla.

![gpt3-01-02](https://user-images.githubusercontent.com/12854504/88138479-5abad680-cbb3-11ea-9463-11a2d1e02f8c.png)


**Figura 1.2: Los modelos más grandes hacen un uso cada vez más eficiente de la información en contexto.** Mostramos el rendimiento de aprendizaje en contexto sobre una tarea simple que requiere que el modelo elimine símbolos aleatorios de una palabra, con o sin descripción de la tarea en lenguaje anatómico (ver Sección [3.9. 2](https://arxiv.org/pdf/2005.14165.pdf#subsubsection.3.9.2)) Las "curvas de aprendizaje en contexto" más pronunciadas para modelos grandes demuestran una capacidad mejorada para aprender una tarea a partir de información contextual. Vemos un comportamiento cualitativamente similar en una amplia gama de tareas. 



Una ruta potencial para abordar estos problemas es el meta-aprendizaje1, que en el contexto de los modelos de lenguaje significa que el modelo desarrolla un amplio conjunto de habilidades y reconocimiento de patrones. habilidades en el momento del entrenamiento, y luego usa esas habilidades en el tiempo de inferencia para adaptarse rápidamente o reconocer la tarea deseada (ilustrada en la Figura 1.1). El trabajo reciente [RWC + 19] intenta hacer esto a través de lo que llamamos "aprendizaje en contexto", utilizando la entrada de texto de un modelo de lenguaje pre-entrenado como una forma de especificación de tareas: el modelo está condicionado a una instrucción de lenguaje natural y / o un Se espera que pocas demostraciones de la tarea completen más instancias de la tarea simplemente al predecir lo que viene a continuación. 

Si bien ha mostrado alguna promesa inicial, este enfoque aún logra resultados muy inferiores a la puesta a punto, por ejemplo [RWC + 19] logra solo el 4% en preguntas naturales, e incluso su resultado de 55 CoQa F1 está ahora más de 35 puntos por detrás del estado del arte. El metaaprendizaje claramente requiere una mejora sustancial para ser viable como un método práctico para resolver tareas del lenguaje. 

Otra tendencia reciente en el modelado del lenguaje puede ofrecer un camino a seguir. En los últimos años, la capacidad de los modelos de lenguaje de transformador ha aumentado sustancialmente, de 100 millones de parámetros [RNSS18], a 300 millones de parámetros [DCLT18], a 1,5 mil millones de parámetros [RWC + 19], a 8 mil millones de parámetros [SPP + 19], 11 mil millones parámetros [RSR + 19], y finalmente 17 mil millones de parámetros [Tur20]. Cada aumento ha traído mejoras en la síntesis de texto y / o en las tareas PNL posteriores, y hay evidencia que sugiere que la pérdida de registros, que se correlaciona bien con muchas tareas posteriores, sigue una tendencia suave de mejora con la escala [KMH + 20]. Dado que el aprendizaje en contexto implica absorber muchas habilidades y tareas dentro de los parámetros del modelo, es plausible que las habilidades de aprendizaje en contexto muestren ganancias similares con escala.

#### [1] En el contexto de los modelos de lenguaje, esto a veces se ha denominado "transferencia de tiro cero" , pero este término es potencialmente ambiguo: el método es "cero" en el sentido de que no se realizan actualizaciones de gradiente, pero a menudo implica proporcionar demostraciones de tiempo de inferencia al modelo, por lo que no está realmente aprendiendo de cero ejemplos. Para evitar esta confusión, utilizamos el término "metaaprendizaje" para capturar la estructura del bucle interno / bucle externo del método general, y el término "aprendizaje en contexto" para referirnos al bucle interno del metaaprendizaje. Además, especializamos la descripción a "disparo cero", "disparo único" o "disparo corto", dependiendo de cuántas demostraciones se brinden en el momento de la inferencia. Estos términos tienen la intención de permanecer agnósticos sobre la cuestión de si el modelo aprende nuevas tareas desde cero en el momento de la inferencia o simplemente reconoce los patrones observados durante el entrenamiento; este es un tema importante que discutiremos más adelante en el documento, pero el "meta-aprendizaje" está destinado a abarca ambas posibilidades y simplemente describe la estructura del bucle interno-externo.


![gpt3-01-03](https://user-images.githubusercontent.com/12854504/88138473-57bfe600-cbb3-11ea-89db-9847a1297ee8.png)

**Figura 1.3: Rendimiento agregado para los 42 puntos de referencia denominados con precisión** Mientras que el rendimiento de tiro cero mejora constantemente con el tamaño del modelo, el rendimiento de tiro corto aumenta más rápidamente, lo que demuestra que los modelos más grandes son más competentes en el aprendizaje en contexto. Consulte la Figura 3.8 para obtener un análisis más detallado de SuperGLUE, una suite estándar NLPbenchmark. 

En este documento, probamos esta hipótesis al entrenar un modelo de lenguaje autorregresivo de 175 mil millones de parámetros, que llamamos GPT-3, y medir sus habilidades de aprendizaje en contexto. Específicamente, evaluamos GPT-3 en más de dos docenas de conjuntos de datos de PNL, así como varias tareas novedosas diseñadas para probar la adaptación rápida a tareas que probablemente no estén directamente contenidas en el conjunto de entrenamiento. Para cada tarea, evaluamos GPT-3 bajo 3 condiciones: (a) "aprendizaje de pocos disparos", o aprendizaje en contexto donde permitimos tantas demostraciones como quepan en la ventana de contexto del modelo (típicamente 10 a 100), (b ) "Aprendizaje único", donde permitimos solo una demostración, y (c) aprendizaje "cero-disparo", donde no se permiten demostraciones y solo se da una instrucción en lenguaje natural al modelo. GPT-3 también podría evaluarse en principio en la configuración tradicional de ajuste fino, pero lo dejamos para el trabajo futuro. La Figura 1.2 ilustra las condiciones que estudiamos y muestra el aprendizaje de pocos disparos de una tarea simple que requiere que el modelo elimine símbolos extraños de un palabra. El rendimiento del modelo mejora con la adición de una descripción de tareas en lenguaje natural, y con el número de ejemplos en el contexto del modelo, K. El aprendizaje de pocos disparos también mejora dramáticamente con el tamaño del modelo. Aunque los resultados en este caso son particularmente sorprendentes, las tendencias generales con el tamaño del modelo y la cantidad de ejemplos en contexto se mantienen para la mayoría de las tareas que estudiamos. Hacemos hincapié en que estas curvas de "aprendizaje" implican actualizaciones no graduadas o ajuste fino, solo un número creciente de demostraciones dadas como condicionamiento. En general, en las tareas de PNL GPT-3 logra resultados prometedores en la configuración de disparo único y cero, y en el el conjunto de pocos disparos a veces es competitivo o incluso ocasionalmente supera el estado de la técnica (a pesar de que los modelos afinados mantienen el estado de la técnica). Por ejemplo, GPT-3 logra 81.5 F1 en CoQA en la configuración de disparo cero, 84.0 F1 en CoQA en la configuración de un disparo, 85.0 F1 en la configuración de pocos disparos. De manera similar, GPT-3 logra una precisión de 64.3% en TriviaQA en la configuración de disparo cero, 68.0% en la configuración de disparo único y 71.2% en la configuración de disparos pocos, la última de las cuales es de vanguardia en relación con la multa modelos sintonizados que operan en la misma configuración de libro cerrado. GPT-3 también muestra la habilidad de un disparo y pocos disparos en tareas diseñadas para probar la adaptación rápida o el razonamiento sobre la marcha, que incluyen descifrar palabras, realizar operaciones aritméticas y usar palabras nuevas en una oración después de verlas definidas solo una vez. También mostramos que en el entorno de pocos disparos, GPT-3 puede generar artículos de noticias sintéticos que los evaluadores humanos tienen dificultades para distinguir de los artículos generados por humanos. Al mismo tiempo, también encontramos algunas tareas en las que luchas de rendimiento de pocos disparos, incluso en La escala de GPT-3. Esto incluye tareas de inferencia de lenguaje natural como el conjunto de datos ANLI y algunos conjuntos de datos de comprensión de lectura como RACE o QuAC. Al presentar una caracterización amplia de las fortalezas y debilidades de GPT-3, incluidas estas limitaciones, esperamos estimular el estudio del aprendizaje de pocos disparos en modelos de lenguaje y llamar la atención sobre dónde se necesita más progreso. Se puede ver un sentido heurístico de los resultados generales en Figura 1.3, que agrega las diversas tareas (aunque no debe verse como un punto de referencia riguroso o significativo en sí mismo) .5
También llevamos a cabo un estudio sistemático de "contaminación de datos", un problema cada vez mayor al entrenar modelos de alta capacidad en conjuntos de datos como Common Crawl, que potencialmente puede incluir contenido de conjuntos de datos de prueba simplemente porque dicho contenido a menudo existe en la web. En este artículo desarrollamos herramientas sistemáticas para medir la contaminación de datos y cuantificar sus efectos distorsionadores. Aunque descubrimos que la contaminación de datos tiene un efecto mínimo en el rendimiento de GPT-3 en la mayoría de los conjuntos de datos, identificamos algunos conjuntos de datos en los que podría estar inflando resultados, y no informamos los resultados en estos conjuntos de datos o los notamos con un asterisco, dependiendo de La gravedad Además de todo lo anterior, también entrenamos una serie de modelos más pequeños (que van desde 125 millones de parámetros a 13 mil millones de parámetros) para comparar su rendimiento con GPT-3 en la configuración de cero, uno y pocos disparos. En términos generales, para la mayoría de las tareas encontramos una escala relativamente suave con capacidad de modelo en las tres configuraciones; Un patrón notable es que la brecha entre el rendimiento de cero, uno y pocos disparos a menudo aumenta con la capacidad del modelo, lo que quizás sugiere que los modelos más grandes son meta-aprendices más competentes. Finalmente, dado el amplio espectro de capacidades que muestra GPT-3, discuta las preocupaciones sobre el sesgo, la equidad y los impactos sociales más amplios, e intente un análisis preliminar de las características de GPT-3 a este respecto.de este artículo está organizado de la siguiente manera. En la Sección 2, describimos nuestro enfoque y métodos para entrenar GPT-3 y evaluarlo. La Sección 3 presenta los resultados en la gama completa de tareas en los entornos de disparo cero, uno y pocos disparos. La Sección 4 aborda cuestiones de contaminación de datos (superposición de pruebas de tren). La Sección 5 discute las limitaciones de GPT-3. La Sección 6 discute los impactos más amplios. La Sección 7 revisa el trabajo relacionado y la Sección 8 concluye.
