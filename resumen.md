## Resumen

Trabajos recientes han demostrado grandes avances en muchos _benchmarks_ y tareas de PNL mediante el pre-entrernamiento sobre un gran corpus de texto, seguido de un _fine-tuning_ para una tarea específica. Si bien la arquitectura es típicamente agnostica frente a la tarea, este método aún requiere un afinamiento o _fine-tuning_ para la tarea especifica sobre _datasets_ de miles o decenas de miles de ejemplos. En contraste, los humanos generalmente pueden realizar una nueva tarea de lenguaje a partir de solo algunos ejemplos o instrucciones simples, - algo que los sistemas de PNL actuales todavía tienen muchas dificultades para hacer -. Aquí mostramos como el _escalar _ el tamaño del modelo mejora en gran medida el rendimiento, el perfomance de _few shots_, y a veces incluso alcanzando niveles de competitividad con enfoques de vanguardia con _fine tuning_ anteriores. Específicamente, entrenamos GPT-3, un modelo de lenguaje autoregresivo con 175 mil millones de parámetros, 10 veces más que cualquier modelo de lenguaje _non-sparse_ anterior, y probamos su rendimiento en la configuración _few shots_. Para todas las tareas, se aplica GPT-3 con tareas y demostraciones de _few shots_ especificadas únicamente a través de la interacción del texto con el modelo, sin actualizaciones de gradiente o _fine tuning_. GPT-3 alcanza un alto rendimiento en diferentes _datasets_ de PNL, incluyendo tareas de traducción, responder preguntas, así como varias tareas que requieren razonamiento sobre la marcha o adaptación de dominio, tales como descifrar palabras, usar una palabra nueva en una oración, o realizar aritmética de 3 dígitos. Al mismo tiempo, también identificamos algunos _datasets_ donde el aprendizaje de _few shots_ de GPT-3 todavía tiene dificultades, así como algunos _datasets_ donde GPT-3 enfrenta problemas metodológicos relacionados con el entrenamiento basado en corpora de la web. Finalmente, encontramos que GPT-3 puede generar muestras de artículos de noticias dificiles de distinguir por evaluadores humanos. Discutimos ademas los impactos sociales más amplios de este hallazgo y de GPT-3 en general.
